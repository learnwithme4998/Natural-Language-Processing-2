{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b00b0849",
   "metadata": {},
   "source": [
    "# Extracting the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16df7fb0",
   "metadata": {},
   "source": [
    " - Reading PDF file in Python\n",
    " - Reading word document\n",
    " - Reading JSON object\n",
    " - Reading HTML page and HTML parsing\n",
    " - Regular expressions\n",
    " - String handling\n",
    " - Web scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5575eb34",
   "metadata": {},
   "source": [
    "## Client Data\n",
    "SQL databases\n",
    "1. Hadoop clusters\n",
    "2. Cloud storage\n",
    "3. Flat files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9232a8b",
   "metadata": {},
   "source": [
    "## Free source \n",
    "A huge amount of data is freely available over the\n",
    "internet. We just need to streamline the problem and start exploring\n",
    "multiple free data sources.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a17368c",
   "metadata": {},
   "source": [
    "Free APIs like Twitter\n",
    "1. Wikipedia\n",
    "2. Government data (e.g. http://data.gov)\n",
    "3. Census data (e.g. http://www.census.gov/data.html)\n",
    "4. Health care claim data (e.g. https://www.healthdata.gov/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158595f3",
   "metadata": {},
   "source": [
    "### Web scraping \n",
    "Extracting the content/data from websites, blogs,\n",
    "forums, and retail websites for reviews with the permission from the\n",
    "respective sources using web scraping packages in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aadb2e6",
   "metadata": {},
   "source": [
    "### 1.2 Collecting Data from PDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60694977",
   "metadata": {},
   "source": [
    "The simplest way to do this is by using the PyPDF2 library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "3fe542ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyPDF2 in /Users/jothiramsanjeevi/opt/anaconda3/lib/python3.8/site-packages (3.0.1)\r\n",
      "Requirement already satisfied: typing_extensions>=3.10.0.0 in /Users/jothiramsanjeevi/opt/anaconda3/lib/python3.8/site-packages (from PyPDF2) (4.7.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install PyPDF2\n",
    "import PyPDF2\n",
    "from PyPDF2 import PdfFileReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "f768068e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a pdf file object\n",
    "pdf = open(\"Get_Started_With_Smallpdf.pdf\",\"rb\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "821971aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating pdf reader object\n",
    "pdf_reader = PyPDF2.PdfReader(pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "cf8315f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking number of pages in a pdf file\n",
    "len(pdf_reader.pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "4b962895",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a page object\n",
    "page = pdf_reader.pages[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "7c076f21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'/Annots': [IndirectObject(63, 0, 140653209483968),\n",
       "  IndirectObject(60, 0, 140653209483968),\n",
       "  IndirectObject(15, 0, 140653209483968),\n",
       "  IndirectObject(12, 0, 140653209483968),\n",
       "  IndirectObject(94, 0, 140653209483968)],\n",
       " '/ArtBox': [0, 0, 595.276, 841.89],\n",
       " '/BleedBox': [0, 0, 595.276, 841.89],\n",
       " '/Contents': [IndirectObject(21, 0, 140653209483968),\n",
       "  IndirectObject(22, 0, 140653209483968),\n",
       "  IndirectObject(23, 0, 140653209483968),\n",
       "  IndirectObject(24, 0, 140653209483968),\n",
       "  IndirectObject(25, 0, 140653209483968),\n",
       "  IndirectObject(26, 0, 140653209483968),\n",
       "  IndirectObject(27, 0, 140653209483968),\n",
       "  IndirectObject(28, 0, 140653209483968)],\n",
       " '/CropBox': [0, 0, 595.276, 841.89],\n",
       " '/Group': {'/CS': ['/ICCBased', IndirectObject(29, 0, 140653209483968)],\n",
       "  '/S': '/Transparency',\n",
       "  '/Type': '/Group'},\n",
       " '/MediaBox': [0, 0, 595.276, 841.89],\n",
       " '/Parent': {'/Count': 1,\n",
       "  '/Kids': [IndirectObject(10, 0, 140653209483968)],\n",
       "  '/Type': '/Pages'},\n",
       " '/PieceInfo': {'/InDesign': {'/DocumentID': 'xmp.did:b47e2f57-0029-45c5-8e1d-97f7c1535615',\n",
       "   '/LastModified': 'D:20201014150810Z',\n",
       "   '/NumberofPages': 1,\n",
       "   '/OriginalDocumentID': 'xmp.did:d36a16ec-5a63-4b4a-81f5-1cc9f49b1966',\n",
       "   '/PageUIDList': {'/0': 211},\n",
       "   '/PageWidthList': {'/0': 595.276}}},\n",
       " '/Resources': {'/ColorSpace': {'/CS0': ['/ICCBased',\n",
       "    IndirectObject(29, 0, 140653209483968)]},\n",
       "  '/ExtGState': {'/GS0': {'/AIS': False,\n",
       "    '/BM': '/Normal',\n",
       "    '/CA': 1,\n",
       "    '/OP': False,\n",
       "    '/OPM': 1,\n",
       "    '/SA': True,\n",
       "    '/SMask': '/None',\n",
       "    '/Type': '/ExtGState',\n",
       "    '/ca': 1,\n",
       "    '/op': False},\n",
       "   '/GS1': {'/AIS': False,\n",
       "    '/BM': '/Normal',\n",
       "    '/CA': 0.800003,\n",
       "    '/OP': False,\n",
       "    '/OPM': 1,\n",
       "    '/SA': True,\n",
       "    '/SMask': '/None',\n",
       "    '/Type': '/ExtGState',\n",
       "    '/ca': 0.800003,\n",
       "    '/op': False},\n",
       "   '/GS2': {'/AIS': False,\n",
       "    '/BM': '/Normal',\n",
       "    '/CA': 0.198654,\n",
       "    '/OP': False,\n",
       "    '/OPM': 1,\n",
       "    '/SA': True,\n",
       "    '/SMask': '/None',\n",
       "    '/Type': '/ExtGState',\n",
       "    '/ca': 0.198654,\n",
       "    '/op': False},\n",
       "   '/GS3': {'/AIS': False,\n",
       "    '/BM': '/Normal',\n",
       "    '/CA': 0.100006,\n",
       "    '/OP': False,\n",
       "    '/OPM': 1,\n",
       "    '/SA': True,\n",
       "    '/SMask': '/None',\n",
       "    '/Type': '/ExtGState',\n",
       "    '/ca': 0.100006,\n",
       "    '/op': False},\n",
       "   '/GS4': {'/AIS': False,\n",
       "    '/BM': '/Normal',\n",
       "    '/CA': 0.960007,\n",
       "    '/OP': False,\n",
       "    '/OPM': 1,\n",
       "    '/SA': True,\n",
       "    '/SMask': '/None',\n",
       "    '/Type': '/ExtGState',\n",
       "    '/ca': 0.960007,\n",
       "    '/op': False},\n",
       "   '/GS5': {'/AIS': False,\n",
       "    '/BM': '/Normal',\n",
       "    '/CA': 0.119995,\n",
       "    '/OP': False,\n",
       "    '/OPM': 1,\n",
       "    '/SA': True,\n",
       "    '/SMask': '/None',\n",
       "    '/Type': '/ExtGState',\n",
       "    '/ca': 0.119995,\n",
       "    '/op': False}},\n",
       "  '/Font': {'/C2_0': {'/BaseFont': '/JFGNNE+SourceSansPro-Regular',\n",
       "    '/DescendantFonts': [IndirectObject(40, 0, 140653209483968)],\n",
       "    '/Encoding': '/Identity-H',\n",
       "    '/Subtype': '/Type0',\n",
       "    '/ToUnicode': {'/Filter': '/FlateDecode'},\n",
       "    '/Type': '/Font'},\n",
       "   '/TT0': {'/BaseFont': '/JFGNNE+SourceSansPro-Bold',\n",
       "    '/Encoding': '/WinAnsiEncoding',\n",
       "    '/FirstChar': 32,\n",
       "    '/FontDescriptor': {'/Ascent': 1009,\n",
       "     '/CapHeight': 660,\n",
       "     '/Descent': -315,\n",
       "     '/Flags': 32,\n",
       "     '/FontBBox': [-457, -315, 2157, 1009],\n",
       "     '/FontFamily': 'Source Sans Pro',\n",
       "     '/FontFile2': {'/Filter': '/FlateDecode', '/Length1': 26053},\n",
       "     '/FontName': '/JFGNNE+SourceSansPro-Bold',\n",
       "     '/FontStretch': '/Normal',\n",
       "     '/FontWeight': 700,\n",
       "     '/ItalicAngle': 0,\n",
       "     '/StemV': 148,\n",
       "     '/Type': '/FontDescriptor',\n",
       "     '/XHeight': 496},\n",
       "    '/LastChar': 151,\n",
       "    '/Subtype': '/TrueType',\n",
       "    '/ToUnicode': {'/Filter': '/FlateDecode'},\n",
       "    '/Type': '/Font',\n",
       "    '/Widths': [200,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     300,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     573,\n",
       "     0,\n",
       "     582,\n",
       "     635,\n",
       "     548,\n",
       "     524,\n",
       "     0,\n",
       "     0,\n",
       "     301,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     684,\n",
       "     596,\n",
       "     0,\n",
       "     0,\n",
       "     556,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     813,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     527,\n",
       "     573,\n",
       "     467,\n",
       "     573,\n",
       "     518,\n",
       "     341,\n",
       "     534,\n",
       "     571,\n",
       "     276,\n",
       "     0,\n",
       "     548,\n",
       "     286,\n",
       "     857,\n",
       "     572,\n",
       "     555,\n",
       "     573,\n",
       "     0,\n",
       "     398,\n",
       "     443,\n",
       "     383,\n",
       "     568,\n",
       "     0,\n",
       "     776,\n",
       "     0,\n",
       "     521,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     800]},\n",
       "   '/TT1': {'/BaseFont': '/JFGNNE+SourceSansPro-Regular',\n",
       "    '/Encoding': '/WinAnsiEncoding',\n",
       "    '/FirstChar': 32,\n",
       "    '/FontDescriptor': {'/Ascent': 968,\n",
       "     '/CapHeight': 660,\n",
       "     '/Descent': -292,\n",
       "     '/Flags': 32,\n",
       "     '/FontBBox': [-454, -292, 2159, 968],\n",
       "     '/FontFamily': 'Source Sans Pro',\n",
       "     '/FontFile2': {'/Filter': '/FlateDecode', '/Length1': 27007},\n",
       "     '/FontName': '/JFGNNE+SourceSansPro-Regular',\n",
       "     '/FontStretch': '/Normal',\n",
       "     '/FontWeight': 400,\n",
       "     '/ItalicAngle': 0,\n",
       "     '/StemV': 84,\n",
       "     '/Type': '/FontDescriptor',\n",
       "     '/XHeight': 486},\n",
       "    '/LastChar': 146,\n",
       "    '/Subtype': '/TrueType',\n",
       "    '/ToUnicode': {'/Filter': '/FlateDecode'},\n",
       "    '/Type': '/Font',\n",
       "    '/Widths': [200,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     249,\n",
       "     311,\n",
       "     249,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     425,\n",
       "     0,\n",
       "     544,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     494,\n",
       "     617,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     727,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     569,\n",
       "     534,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     786,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     504,\n",
       "     553,\n",
       "     456,\n",
       "     555,\n",
       "     496,\n",
       "     292,\n",
       "     504,\n",
       "     544,\n",
       "     246,\n",
       "     0,\n",
       "     495,\n",
       "     255,\n",
       "     829,\n",
       "     547,\n",
       "     542,\n",
       "     555,\n",
       "     555,\n",
       "     347,\n",
       "     419,\n",
       "     338,\n",
       "     544,\n",
       "     467,\n",
       "     718,\n",
       "     446,\n",
       "     467,\n",
       "     425,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     249,\n",
       "     249]}},\n",
       "  '/ProcSet': ['/PDF', '/Text'],\n",
       "  '/XObject': {'/Fm0': {'/BBox': [-32768, 32767, 32767, -32768],\n",
       "    '/Filter': '/FlateDecode',\n",
       "    '/Group': {'/I': True,\n",
       "     '/K': False,\n",
       "     '/S': '/Transparency',\n",
       "     '/Type': '/Group'},\n",
       "    '/Matrix': [1, 0, 0, 1, 0, 0],\n",
       "    '/Resources': {'/ColorSpace': {'/CS0': ['/ICCBased',\n",
       "       IndirectObject(29, 0, 140653209483968)]},\n",
       "     '/ExtGState': {'/GS0': {'/AIS': False,\n",
       "       '/BM': '/Normal',\n",
       "       '/CA': 1,\n",
       "       '/OP': False,\n",
       "       '/OPM': 1,\n",
       "       '/SA': True,\n",
       "       '/SMask': '/None',\n",
       "       '/Type': '/ExtGState',\n",
       "       '/ca': 1,\n",
       "       '/op': False}}},\n",
       "    '/Subtype': '/Form'},\n",
       "   '/Fm1': {'/BBox': [-32768, 32767, 32767, -32768],\n",
       "    '/Filter': '/FlateDecode',\n",
       "    '/Group': {'/I': True,\n",
       "     '/K': False,\n",
       "     '/S': '/Transparency',\n",
       "     '/Type': '/Group'},\n",
       "    '/Matrix': [1, 0, 0, 1, 0, 0],\n",
       "    '/Resources': {'/ColorSpace': {'/CS0': ['/ICCBased',\n",
       "       IndirectObject(29, 0, 140653209483968)]},\n",
       "     '/ExtGState': {'/GS0': {'/AIS': False,\n",
       "       '/BM': '/Normal',\n",
       "       '/CA': 1,\n",
       "       '/OP': False,\n",
       "       '/OPM': 1,\n",
       "       '/SA': True,\n",
       "       '/SMask': '/None',\n",
       "       '/Type': '/ExtGState',\n",
       "       '/ca': 1,\n",
       "       '/op': False}}},\n",
       "    '/Subtype': '/Form'},\n",
       "   '/Fm2': {'/BBox': [-32768, 32767, 32767, -32768],\n",
       "    '/Filter': '/FlateDecode',\n",
       "    '/Group': {'/I': True,\n",
       "     '/K': False,\n",
       "     '/S': '/Transparency',\n",
       "     '/Type': '/Group'},\n",
       "    '/Matrix': [1, 0, 0, 1, 0, 0],\n",
       "    '/Resources': {'/ColorSpace': {'/CS0': ['/ICCBased',\n",
       "       IndirectObject(29, 0, 140653209483968)]},\n",
       "     '/ExtGState': {'/GS0': {'/AIS': False,\n",
       "       '/BM': '/Normal',\n",
       "       '/CA': 1,\n",
       "       '/OP': False,\n",
       "       '/OPM': 1,\n",
       "       '/SA': True,\n",
       "       '/SMask': '/None',\n",
       "       '/Type': '/ExtGState',\n",
       "       '/ca': 1,\n",
       "       '/op': False}}},\n",
       "    '/Subtype': '/Form'},\n",
       "   '/Fm3': {'/BBox': [-32768, 32767, 32767, -32768],\n",
       "    '/Filter': '/FlateDecode',\n",
       "    '/Group': {'/I': True,\n",
       "     '/K': False,\n",
       "     '/S': '/Transparency',\n",
       "     '/Type': '/Group'},\n",
       "    '/Matrix': [1, 0, 0, 1, 0, 0],\n",
       "    '/Resources': {'/ColorSpace': {'/CS0': ['/ICCBased',\n",
       "       IndirectObject(29, 0, 140653209483968)]},\n",
       "     '/ExtGState': {'/GS0': {'/AIS': False,\n",
       "       '/BM': '/Normal',\n",
       "       '/CA': 1,\n",
       "       '/OP': False,\n",
       "       '/OPM': 1,\n",
       "       '/SA': True,\n",
       "       '/SMask': '/None',\n",
       "       '/Type': '/ExtGState',\n",
       "       '/ca': 1,\n",
       "       '/op': False}}},\n",
       "    '/Subtype': '/Form'},\n",
       "   '/Fm4': {'/BBox': [-32768, 32767, 32767, -32768],\n",
       "    '/Filter': '/FlateDecode',\n",
       "    '/Group': {'/I': True,\n",
       "     '/K': False,\n",
       "     '/S': '/Transparency',\n",
       "     '/Type': '/Group'},\n",
       "    '/Matrix': [1, 0, 0, 1, 0, 0],\n",
       "    '/Resources': {'/ColorSpace': {'/CS0': ['/ICCBased',\n",
       "       IndirectObject(29, 0, 140653209483968)]},\n",
       "     '/ExtGState': {'/GS0': {'/AIS': False,\n",
       "       '/BM': '/Normal',\n",
       "       '/CA': 1,\n",
       "       '/OP': False,\n",
       "       '/OPM': 1,\n",
       "       '/SA': True,\n",
       "       '/SMask': '/None',\n",
       "       '/Type': '/ExtGState',\n",
       "       '/ca': 1,\n",
       "       '/op': False}}},\n",
       "    '/Subtype': '/Form'}}},\n",
       " '/Rotate': 0,\n",
       " '/Tabs': '/W',\n",
       " '/Thumb': {'/BitsPerComponent': 8,\n",
       "  '/ColorSpace': ['/Indexed',\n",
       "   '/DeviceRGB',\n",
       "   255,\n",
       "   IndirectObject(1, 0, 140653209483968)],\n",
       "  '/Filter': ['/ASCII85Decode', '/FlateDecode'],\n",
       "  '/Height': 105,\n",
       "  '/Width': 74},\n",
       " '/TrimBox': [0, 0, 595.276, 841.89],\n",
       " '/Type': '/Page'}"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_reader.pages[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "0fc4b7c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to Smallpdf\n",
      "Digital Documents—All In One Place\n",
      "Access Files Anytime, Anywhere Enhance Documents in One Click \n",
      "Collaborate With Others With the new Smallpdf experience, you can \n",
      "freely upload, organize, and share digital \n",
      "documents. When you enable the ‘Storage’ \n",
      "option, we’ll also store all processed files here. \n",
      "You can access files stored on Smallpdf from \n",
      "your computer, phone, or tablet. We’ll also \n",
      "sync files from the Smallpdf Mobile App to our \n",
      "online portalWhen you right-click on a file, we’ll present \n",
      "you with an array of options to convert, \n",
      "compress, or modify it. \n",
      "Forget mundane administrative tasks. With \n",
      "Smallpdf, you can request e-signatures, send \n",
      "large files, or even enable the Smallpdf G Suite \n",
      "App for your entire organization. Ready to take document management to the next level? \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#finally extracting text from the page\n",
    "print(page.extract_text())\n",
    "#closing the pdf file\n",
    "pdf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ae7d4d",
   "metadata": {},
   "source": [
    "### 1-3. Collecting Data from Word Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "0d42e919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-docx in /Users/jothiramsanjeevi/opt/anaconda3/lib/python3.8/site-packages (0.8.11)\r\n",
      "Requirement already satisfied: lxml>=2.3.2 in /Users/jothiramsanjeevi/opt/anaconda3/lib/python3.8/site-packages (from python-docx) (4.6.3)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install python-docx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "6d208dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "\n",
    "# Path to the Word document\n",
    "document_path = '/Users/jothiramsanjeevi/Documents/IPythonnotebook/Natural Language Processing Bootcamp/Sample.docx'\n",
    "\n",
    "# Open the Word document\n",
    "doc = Document(document_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "34dcab2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text from paragraphs\n",
    "text = []\n",
    "for paragraph in doc.paragraphs:\n",
    "    text.append(paragraph.text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "1b371992",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The majestic mountains stood tall, their peaks reaching towards the heavens. The lush green valleys sprawled beneath, adorned with colorful wildflowers that swayed gently in the breeze. Sunlight cascaded through the gaps between the trees, painting dappled patterns on the forest floor. Birds chirped melodiously, their songs echoing through the serene landscape. Nature\\'s orchestra played harmoniously, creating a symphony of sights and sounds. It was a place where time seemed to stand still, where one could lose themselves in the beauty and tranquility of the natural world.\"']"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "6b92018a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the extracted text into a single string\n",
    "extracted_text = '\\n'.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "6bda3aa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(extracted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "a3368404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The majestic mountains stood tall, their peaks reaching towards the heavens. The lush green valleys sprawled beneath, adorned with colorful wildflowers that swayed gently in the breeze. Sunlight cascaded through the gaps between the trees, painting dappled patterns on the forest floor. Birds chirped melodiously, their songs echoing through the serene landscape. Nature's orchestra played harmoniously, creating a symphony of sights and sounds. It was a place where time seemed to stand still, where one could lose themselves in the beauty and tranquility of the natural world.\"\n"
     ]
    }
   ],
   "source": [
    "# Print or use the extracted text as needed\n",
    "print(extracted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "3c99caeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(extracted_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2d5297",
   "metadata": {},
   "source": [
    "### 1-4. Collecting Data from JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "b3734437",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Path to the JSON file\n",
    "json_file_path = '/Users/jothiramsanjeevi/Documents/IPythonnotebook/Natural Language Processing Bootcamp/example_1.json'\n",
    "\n",
    "# Open the JSON file\n",
    "with open(json_file_path) as file:\n",
    "    data = json.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "062dd4f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple\n"
     ]
    }
   ],
   "source": [
    "# Extract text from specific fields or keys in the JSON data\n",
    "text = data['fruit']\n",
    "\n",
    "# Print or use the extracted text as needed\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112ccefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "eaf4270c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"message\": \"Not authenticated\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#json from \"https://quotes.rest/qod.json\"\n",
    "#https://theysaidso.com/login\n",
    "    \n",
    "r = requests.get(\"https://quotes.rest/qod.json\")\n",
    "res = r.json()\n",
    "print(json.dumps(res, indent = 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "312794b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# API endpoint\n",
    "url = 'https://quotes.rest/qod.json'\n",
    "\n",
    "# API key\n",
    "api_key = 'haB3xY8M7T16YpIB5Qliotrqdz2Ef0FKFdWDM6QA'\n",
    "\n",
    "# Set the headers with the API key\n",
    "headers = {\n",
    "    'X-TheySaidSo-Api-Secret': api_key\n",
    "}\n",
    "\n",
    "# Send GET request to the API with headers\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Extract and process the response data\n",
    "data = response.json()\n",
    "# Process the data as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "c449ee2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'success': {'total': 1},\n",
       " 'contents': {'quotes': [{'id': 'wPkodOctkz8HYuyIo1e8FgeF',\n",
       "    'quote': 'Winning is not everything, but the effort to win is.',\n",
       "    'length': 52,\n",
       "    'author': 'Zig Ziglar',\n",
       "    'language': 'en',\n",
       "    'tags': ['effort', 'inspire', 'winning', 'win'],\n",
       "    'sfw': 'sfw',\n",
       "    'permalink': 'https://theysaidso.com/quote/zig-ziglar-winning-is-not-everything-but-the-effort-to-win-is',\n",
       "    'title': 'Inspiring Quote of the day',\n",
       "    'category': 'inspire',\n",
       "    'background': 'https://theysaidso.com/assets/images/qod/qod-inspire.jpg',\n",
       "    'date': '2023-07-20'}]},\n",
       " 'copyright': {'url': 'https://quotes.rest', 'year': '2023'}}"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "3403094d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"success\": {\n",
      "        \"total\": 1\n",
      "    },\n",
      "    \"contents\": {\n",
      "        \"quotes\": [\n",
      "            {\n",
      "                \"id\": \"wPkodOctkz8HYuyIo1e8FgeF\",\n",
      "                \"quote\": \"Winning is not everything, but the effort to win is.\",\n",
      "                \"length\": 52,\n",
      "                \"author\": \"Zig Ziglar\",\n",
      "                \"language\": \"en\",\n",
      "                \"tags\": [\n",
      "                    \"effort\",\n",
      "                    \"inspire\",\n",
      "                    \"winning\",\n",
      "                    \"win\"\n",
      "                ],\n",
      "                \"sfw\": \"sfw\",\n",
      "                \"permalink\": \"https://theysaidso.com/quote/zig-ziglar-winning-is-not-everything-but-the-effort-to-win-is\",\n",
      "                \"title\": \"Inspiring Quote of the day\",\n",
      "                \"category\": \"inspire\",\n",
      "                \"background\": \"https://theysaidso.com/assets/images/qod/qod-inspire.jpg\",\n",
      "                \"date\": \"2023-07-20\"\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    \"copyright\": {\n",
      "        \"url\": \"https://quotes.rest\",\n",
      "        \"year\": \"2023\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(data, indent = 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "53b19172",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'wPkodOctkz8HYuyIo1e8FgeF',\n",
       " 'quote': 'Winning is not everything, but the effort to win is.',\n",
       " 'length': 52,\n",
       " 'author': 'Zig Ziglar',\n",
       " 'language': 'en',\n",
       " 'tags': ['effort', 'inspire', 'winning', 'win'],\n",
       " 'sfw': 'sfw',\n",
       " 'permalink': 'https://theysaidso.com/quote/zig-ziglar-winning-is-not-everything-but-the-effort-to-win-is',\n",
       " 'title': 'Inspiring Quote of the day',\n",
       " 'category': 'inspire',\n",
       " 'background': 'https://theysaidso.com/assets/images/qod/qod-inspire.jpg',\n",
       " 'date': '2023-07-20'}"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extract contents\n",
    "q = data['contents']['quotes'][0]\n",
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "71e79f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Winning is not everything, but the effort to win is. \n",
      "-- Zig Ziglar\n"
     ]
    }
   ],
   "source": [
    "#extract only quote\n",
    "print(q['quote'], '\\n--', q['author'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c87a0a",
   "metadata": {},
   "source": [
    "### 1-5. Collecting Data from HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b85b234",
   "metadata": {},
   "source": [
    "\n",
    "Parsing text refers to the process of analyzing and interpreting the structure and meaning of text data. In the context of programming, parsing is commonly used when working with structured data formats like HTML, XML, JSON, or specific file formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "b649dc85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in /Users/jothiramsanjeevi/opt/anaconda3/lib/python3.8/site-packages (0.0.1)\r\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/jothiramsanjeevi/opt/anaconda3/lib/python3.8/site-packages (from bs4) (4.9.3)\r\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/jothiramsanjeevi/opt/anaconda3/lib/python3.8/site-packages (from beautifulsoup4->bs4) (2.2.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install bs4\n",
    "import urllib.request as urllib2\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "4f10eacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = urllib2.urlopen('https://en.wikipedia.org/wiki/Natural_language_processing')\n",
    "html_doc = response.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "9f5ec33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html class=\"client-nojs vector-feature-language-in-header-enabled vector-feature-language-in-main-page-header-disabled vector-feature-sticky-header-disabled vector-feature-page-tools-pinned-disabled vector-feature-toc-pinned-enabled vector-feature-main-menu-pinned-disabled vector-feature-limited-width-enabled vector-feature-limited-width-content-enabled vector-feature-zebra-design-disabled\" dir=\"ltr\" lang=\"en\">\n",
      " <head>\n",
      "  <meta charset=\"utf-8\"/>\n",
      "  <title>\n",
      "   Natural language processing - Wikipedia\n",
      "  </title>\n",
      "  <script>\n",
      "   document.documentElement.className=\"client-js vector-feature-language-in-header-enabled vector-feature-language-in-main-page-header-disabled vector-feature-sticky-header-disabled vector-feature-page-tools-pinned-disabled vector-feature-toc-pinned-enabled vector-feature-main-menu-pinned-disabled vector-feature-limited-width-enabled vector-feature-limited-width-content-enabled vector-feature-zebra-design-disabled\";(function(){var cookie=document.cookie.m\n"
     ]
    }
   ],
   "source": [
    "#Parsing\n",
    "soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "# Formating the parsed html file\n",
    "strhtm = soup.prettify()\n",
    "# Print few lines\n",
    "print (strhtm[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039b75b6",
   "metadata": {},
   "source": [
    "Extracting tag value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "29a48b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>Natural language processing - Wikipedia</title>\n",
      "Natural language processing - Wikipedia\n",
      "Jump to content\n",
      "Natural language processing\n"
     ]
    }
   ],
   "source": [
    "print(soup.title)\n",
    "print(soup.title.string)\n",
    "print(soup.a.string)\n",
    "print(soup.b.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17433892",
   "metadata": {},
   "source": [
    "Extracting all instances of a particular tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "2f19c3b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jump to content\n",
      "Main page\n",
      "Contents\n",
      "Current events\n",
      "Random article\n",
      "About Wikipedia\n",
      "Contact us\n",
      "Donate\n",
      "Help\n",
      "Learn to edit\n",
      "Community portal\n",
      "Recent changes\n",
      "Upload file\n",
      "None\n",
      "None\n",
      "Create account\n",
      "Log in\n",
      "None\n",
      "None\n",
      "learn more\n",
      "Contributions\n",
      "Talk\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "Afrikaans\n",
      "العربية\n",
      "Արեւմտահայերէն\n",
      "Azərbaycanca\n",
      "বাংলা\n",
      "Bân-lâm-gú\n",
      "Беларуская\n",
      "Беларуская (тарашкевіца)\n",
      "Български\n",
      "Bosanski\n",
      "Català\n",
      "Čeština\n",
      "Cymraeg\n",
      "Dansk\n",
      "Deutsch\n",
      "Eesti\n",
      "Ελληνικά\n",
      "Español\n",
      "Esperanto\n",
      "Euskara\n",
      "فارسی\n",
      "Français\n",
      "Galego\n",
      "한국어\n",
      "Հայերեն\n",
      "हिन्दी\n",
      "Hrvatski\n",
      "Bahasa Indonesia\n",
      "IsiZulu\n",
      "Íslenska\n",
      "Italiano\n",
      "עברית\n",
      "ಕನ್ನಡ\n",
      "ქართული\n",
      "Latviešu\n",
      "Lietuvių\n",
      "Македонски\n",
      "मराठी\n",
      "مصرى\n",
      "Монгол\n",
      "မြန်မာဘာသာ\n",
      "日本語\n",
      "ଓଡ଼ିଆ\n",
      "Picard\n",
      "Piemontèis\n",
      "Polski\n",
      "Português\n",
      "Română\n",
      "Runa Simi\n",
      "Русский\n",
      "Shqip\n",
      "Simple English\n",
      "کوردی\n",
      "Српски / srpski\n",
      "Srpskohrvatski / српскохрватски\n",
      "Suomi\n",
      "தமிழ்\n",
      "తెలుగు\n",
      "ไทย\n",
      "Türkçe\n",
      "Українська\n",
      "Tiếng Việt\n",
      "粵語\n",
      "中文\n",
      "Edit links\n",
      "Article\n",
      "Talk\n",
      "Read\n",
      "Edit\n",
      "View history\n",
      "Read\n",
      "Edit\n",
      "View history\n",
      "What links here\n",
      "Related changes\n",
      "Upload file\n",
      "Special pages\n",
      "Permanent link\n",
      "Page information\n",
      "Cite this page\n",
      "Wikidata item\n",
      "Download as PDF\n",
      "Printable version\n",
      "Wikimedia Commons\n",
      "Wikiversity\n",
      "Language processing in the brain\n",
      "interdisciplinary\n",
      "linguistics\n",
      "computer science\n",
      "artificial intelligence\n",
      "natural language\n",
      "contextual\n",
      "speech recognition\n",
      "natural-language understanding\n",
      "natural-language generation\n",
      "edit\n",
      "History of natural language processing\n",
      "Alan Turing\n",
      "Computing Machinery and Intelligence\n",
      "Turing test\n",
      "edit\n",
      "John Searle\n",
      "Chinese room\n",
      "Georgetown experiment\n",
      "automatic translation\n",
      "[1]\n",
      "ALPAC report\n",
      "[2]\n",
      "statistical machine translation\n",
      "SHRDLU\n",
      "blocks worlds\n",
      "ELIZA\n",
      "Rogerian psychotherapist\n",
      "Joseph Weizenbaum\n",
      "Ross Quillian\n",
      "[3]\n",
      "ontologies\n",
      "chatterbots\n",
      "PARRY\n",
      "HPSG\n",
      "generative grammar\n",
      "[4]\n",
      "Lesk algorithm\n",
      "[5]\n",
      "Rhetorical Structure Theory\n",
      "Racter\n",
      "Jabberwacky\n",
      "[6]\n",
      "edit\n",
      "machine learning\n",
      "Moore's law\n",
      "Chomskyan\n",
      "transformational grammar\n",
      "corpus linguistics\n",
      "[7]\n",
      "machine translation\n",
      "IBM alignment models\n",
      "textual corpora\n",
      "Parliament of Canada\n",
      "European Union\n",
      "unsupervised\n",
      "semi-supervised learning\n",
      "supervised learning\n",
      "World Wide Web\n",
      "time complexity\n",
      "edit\n",
      "Yoshua Bengio\n",
      "multi-layer perceptron\n",
      "language modelling\n",
      "n-gram models\n",
      "[8]\n",
      "Tomáš Mikolov\n",
      "Brno University of Technology\n",
      "recurrent neural network\n",
      "[9]\n",
      "Word2vec\n",
      "representation learning\n",
      "deep neural network\n",
      "[10]\n",
      "[11]\n",
      "language modeling\n",
      "[12]\n",
      "[13]\n",
      "[14]\n",
      "in medicine and healthcare\n",
      "electronic health records\n",
      "[15]\n",
      "[16]\n",
      "edit\n",
      "[17]\n",
      "[18]\n",
      "stemming\n",
      "Machine learning\n",
      "language models\n",
      "intractability\n",
      "LLMs\n",
      "Apertium\n",
      "tokenization\n",
      "knowledge extraction\n",
      "edit\n",
      "AI winter\n",
      "[19]\n",
      "[20]\n",
      "decision trees\n",
      "if–then rules\n",
      "edit\n",
      "Artificial neural network\n",
      "[21]\n",
      "neural networks\n",
      "word embeddings\n",
      "Neural machine translation\n",
      "sequence-to-sequence\n",
      "statistical machine translation\n",
      "edit\n",
      "edit\n",
      "Optical character recognition\n",
      "Speech recognition\n",
      "text to speech\n",
      "AI-complete\n",
      "natural speech\n",
      "speech segmentation\n",
      "coarticulation\n",
      "analog signal\n",
      "Speech segmentation\n",
      "speech recognition\n",
      "Text-to-speech\n",
      "[22]\n",
      "Word segmentation\n",
      "Tokenization\n",
      "English\n",
      "Chinese\n",
      "Japanese\n",
      "Thai\n",
      "vocabulary\n",
      "morphology\n",
      "bag of words\n",
      "edit\n",
      "Lemmatization\n",
      "[23]\n",
      "Morphological segmentation\n",
      "morphemes\n",
      "morphology\n",
      "English\n",
      "inflectional morphology\n",
      "Turkish\n",
      "Meitei\n",
      "agglutinated\n",
      "[24]\n",
      "Part-of-speech tagging\n",
      "part of speech\n",
      "noun\n",
      "verb\n",
      "adjective\n",
      "Stemming\n",
      "edit\n",
      "Grammar induction\n",
      "[25]\n",
      "formal grammar\n",
      "Sentence breaking\n",
      "sentence boundary disambiguation\n",
      "periods\n",
      "punctuation marks\n",
      "abbreviations\n",
      "Parsing\n",
      "parse tree\n",
      "grammar\n",
      "natural languages\n",
      "ambiguous\n",
      "probabilistic context-free grammar\n",
      "stochastic grammar\n",
      "edit\n",
      "Lexical semantics\n",
      "Distributional semantics\n",
      "Named entity recognition\n",
      "capitalization\n",
      "named entity\n",
      "Chinese\n",
      "Arabic\n",
      "German\n",
      "nouns\n",
      "French\n",
      "Spanish\n",
      "adjectives\n",
      "[26]\n",
      "Sentiment analysis\n",
      "Multimodal sentiment analysis\n",
      "Terminology extraction\n",
      "Word-sense disambiguation\n",
      "meaning\n",
      "WordNet\n",
      "Entity linking\n",
      "named entities\n",
      "edit\n",
      "Relationship extraction\n",
      "Semantic parsing\n",
      "AMR parsing\n",
      "DRT parsing\n",
      "Natural language understanding\n",
      "Semantic role labelling\n",
      "frames\n",
      "semantic roles\n",
      "edit\n",
      "Coreference resolution\n",
      "Anaphora resolution\n",
      "pronouns\n",
      "referring expressions\n",
      "Discourse analysis\n",
      "discourse\n",
      "speech acts\n",
      "frames\n",
      "Semantic role labelling\n",
      "pro-drop languages\n",
      "Recognizing textual entailment\n",
      "[27]\n",
      "Topic segmentation\n",
      "Argument mining\n",
      "natural language\n",
      "[28]\n",
      "argument scheme\n",
      "[29]\n",
      "[30]\n",
      "edit\n",
      "Automatic summarization\n",
      "[31]\n",
      "[32]\n",
      "[33]\n",
      "GPT-2\n",
      "Machine translation\n",
      "AI-complete\n",
      "Natural-language understanding\n",
      "first-order logic\n",
      "computer\n",
      "closed-world assumption\n",
      "open-world assumption\n",
      "[34]\n",
      "Natural-language generation\n",
      "[35]\n",
      "1 the Road\n",
      "language models\n",
      "[36]\n",
      "Document AI\n",
      "[37]\n",
      "Dialogue management\n",
      "Question answering\n",
      "Text-to-image generation\n",
      "[38]\n",
      "3D model\n",
      "[39]\n",
      "[40]\n",
      "[41]\n",
      "[42]\n",
      "edit\n",
      "[43]\n",
      "edit\n",
      "Cognition\n",
      "[44]\n",
      "Cognitive science\n",
      "[45]\n",
      "Cognitive linguistics\n",
      "[46]\n",
      "symbolic NLP\n",
      "George Lakoff\n",
      "cognitive linguistics\n",
      "[47]\n",
      "conceptual metaphor\n",
      "[48]\n",
      "probabilistic context-free grammar\n",
      "US patent 9269353\n",
      "[49]\n",
      "[50]\n",
      "[51]\n",
      "ACT-R\n",
      "[52]\n",
      "ACL\n",
      "explainability\n",
      "[53]\n",
      "multimodal\n",
      "[54]\n",
      "Artificial intelligence\n",
      "Large language model\n",
      "[55]\n",
      "edit\n",
      "1 the Road\n",
      "Artificial intelligence detection software\n",
      "Automated essay scoring\n",
      "Biomedical text mining\n",
      "Compound term processing\n",
      "Computational linguistics\n",
      "Computer-assisted reviewing\n",
      "Controlled natural language\n",
      "Deep learning\n",
      "Deep linguistic processing\n",
      "Distributional semantics\n",
      "Foreign language reading aid\n",
      "Foreign language writing aid\n",
      "Information extraction\n",
      "Information retrieval\n",
      "Language and Communication Technologies\n",
      "Language model\n",
      "Language technology\n",
      "Latent semantic indexing\n",
      "Multi-agent system\n",
      "Native-language identification\n",
      "Natural-language programming\n",
      "Natural-language understanding\n",
      "Natural-language search\n",
      "Outline of natural language processing\n",
      "Query expansion\n",
      "Query understanding\n",
      "Reification (linguistics)\n",
      "Speech processing\n",
      "Spoken dialogue systems\n",
      "Text-proofing\n",
      "Text simplification\n",
      "Transformer (machine learning model)\n",
      "Truecasing\n",
      "Question answering\n",
      "Word2vec\n",
      "edit\n",
      "^\n",
      "\"The history of machine translation in a nutshell\"\n",
      "self-published source\n",
      "^\n",
      "^\n",
      "Crevier 1993\n",
      "help\n",
      "Buchanan 2005\n",
      "help\n",
      "^\n",
      "Koskenniemi, Kimmo\n",
      "Two-level morphology: A general computational model of word-form recognition and production\n",
      "University of Helsinki\n",
      "^\n",
      "Control of Inference: Role of Some Aspects of Discourse Structure-Centering\n",
      "^\n",
      "doi\n",
      "10.1109/PROC.1986.13580\n",
      "ISSN\n",
      "1558-2256\n",
      "S2CID\n",
      "30688575\n",
      "^\n",
      "corner cases\n",
      "pathological\n",
      "thought experiments\n",
      "corpus linguistics\n",
      "corpora\n",
      "poverty of the stimulus\n",
      "^\n",
      "\"A neural probabilistic language model\"\n",
      "^\n",
      "https://gwern.net/doc/ai/nn/rnn/2010-mikolov.pdf\n",
      "^\n",
      "arXiv\n",
      "1807.10854\n",
      "doi\n",
      "10.1613/jair.4992\n",
      "S2CID\n",
      "8273530\n",
      "^\n",
      "Deep Learning\n",
      "^\n",
      "arXiv\n",
      "1602.02410\n",
      "Bibcode\n",
      "2016arXiv160202410J\n",
      "^\n",
      "\"Parsing as Language Modeling\"\n",
      "the original\n",
      "^\n",
      "\"Grammar as a Foreign Language\"\n",
      "arXiv\n",
      "1412.7449\n",
      "Bibcode\n",
      "2014arXiv1412.7449V\n",
      "^\n",
      "\"Using Natural Language Processing to Measure and Improve Quality of Diabetes Care: A Systematic Review\"\n",
      "doi\n",
      "10.1177/19322968211000831\n",
      "ISSN\n",
      "1932-2968\n",
      "PMC\n",
      "8120048\n",
      "PMID\n",
      "33736486\n",
      "^\n",
      "\"Prevalence of Sensitive Terms in Clinical Notes Using Natural Language Processing Techniques: Observational Study\"\n",
      "doi\n",
      "10.2196/38482\n",
      "ISSN\n",
      "2291-9694\n",
      "PMC\n",
      "9233261\n",
      "PMID\n",
      "35687381\n",
      "^\n",
      "Procedures as a Representation for Data in a Computer Program for Understanding Natural Language\n",
      "^\n",
      "ISBN\n",
      "0-470-99033-3\n",
      "^\n",
      "Mark Johnson. How the statistical revolution changes (computational) linguistics.\n",
      "^\n",
      "Philip Resnik. Four revolutions.\n",
      "^\n",
      "\"Deep Learning For NLP-ACL 2012 Tutorial\"\n",
      "http://web.stanford.edu/class/cs224n/\n",
      "^\n",
      "CiteSeerX\n",
      "10.1.1.668.869\n",
      "doi\n",
      "10.1007/978-3-642-29364-1_2\n",
      "ISBN\n",
      "9783642293634\n",
      "^\n",
      "\"What is Natural Language Processing? Intro to NLP in Machine Learning\"\n",
      "^\n",
      "\"Manipuri Morpheme Identification\"\n",
      "cite journal\n",
      "link\n",
      "^\n",
      "\"Natural language grammar induction using a constituent-context model\"\n",
      "^\n",
      "\"Precision information extraction for rare disease epidemiology at scale\"\n",
      "doi\n",
      "10.1186/s12967-023-04011-y\n",
      "PMC\n",
      "9972634\n",
      "PMID\n",
      "36855134\n",
      "^\n",
      "https://tac.nist.gov//2011/RTE/\n",
      "^\n",
      "\"Argumentation Mining: State of the Art and Emerging Trends\"\n",
      "doi\n",
      "10.1145/2850417\n",
      "hdl\n",
      "11585/523460\n",
      "ISSN\n",
      "1533-5399\n",
      "S2CID\n",
      "9561587\n",
      "^\n",
      "\"Argument Mining - IJCAI2016 Tutorial\"\n",
      "^\n",
      "\"NLP Approaches to Computational Argumentation – ACL 2016, Berlin\"\n",
      "^\n",
      "\"Centre for Language Technology (CLT)\"\n",
      "^\n",
      "\"Shared Task: Grammatical Error Correction\"\n",
      "^\n",
      "\"Shared Task: Grammatical Error Correction\"\n",
      "^\n",
      "\"Formalizing Semantic of Natural Language through Conceptualization from Existence\"\n",
      "the original\n",
      "^\n",
      "\"U B U W E B :: Racter\"\n",
      "^\n",
      "doi\n",
      "10.1007/978-3-030-16800-1\n",
      "ISBN\n",
      "978-3-030-16799-8\n",
      "S2CID\n",
      "155818532\n",
      "^\n",
      "\"Document Understanding AI on Google Cloud (Cloud Next '19) - YouTube\"\n",
      "the original\n",
      "^\n",
      "\"OpenAI's DALL-E AI image generator can now edit pictures, too\"\n",
      "^\n",
      "\"The Stanford Natural Language Processing Group\"\n",
      "^\n",
      "\"WordsEye: an automatic text-to-scene conversion system\"\n",
      "doi\n",
      "10.1145/383259.383316\n",
      "ISBN\n",
      "978-1-58113-374-5\n",
      "S2CID\n",
      "3842372\n",
      "^\n",
      "\"Google announces AI advances in text-to-video, language translation, more\"\n",
      "^\n",
      "\"Meta's new text-to-video AI generator is like DALL-E for video\"\n",
      "^\n",
      "\"Previous shared tasks | CoNLL\"\n",
      "^\n",
      "\"Cognition\"\n",
      "Oxford University Press\n",
      "Dictionary.com\n",
      "the original\n",
      "^\n",
      "\"Ask the Cognitive Scientist\"\n",
      "^\n",
      "ISBN\n",
      "978-0-805-85352-0\n",
      "^\n",
      "ISBN\n",
      "978-0-465-05674-3\n",
      "^\n",
      "ISBN\n",
      "978-0-521-59541-4\n",
      "^\n",
      "\"Universal Conceptual Cognitive Annotation (UCCA)\"\n",
      "^\n",
      "Building an RRG computational grammar\n",
      "^\n",
      "\"Fluid Construction Grammar – A fully operational processing system for construction grammars\"\n",
      "^\n",
      "\"ACL Member Portal | The Association for Computational Linguistics Member Portal\"\n",
      "^\n",
      "\"Chunks and Rules\"\n",
      "^\n",
      "\"Grounded Compositional Semantics for Finding and Describing Images with Sentences\"\n",
      "doi\n",
      "10.1162/tacl_a_00177\n",
      "S2CID\n",
      "2317858\n",
      "^\n",
      "arXiv\n",
      "2207.07051\n",
      "cs.CL\n",
      "edit\n",
      "\"Models of natural language understanding\"\n",
      "Bibcode\n",
      "1995PNAS...92.9977B\n",
      "doi\n",
      "10.1073/pnas.92.22.9977\n",
      "PMC\n",
      "40721\n",
      "PMID\n",
      "7479812\n",
      "ISBN\n",
      "978-0-596-51649-9\n",
      "ISBN\n",
      "978-0-13-187321-6\n",
      "ISBN\n",
      "978-1848218482\n",
      "ISBN\n",
      "978-1848219212\n",
      "ISBN\n",
      "978-0-521-86571-5\n",
      "Official html and pdf versions available without charge.\n",
      "ISBN\n",
      "978-0-262-13360-9\n",
      "ISBN\n",
      "978-0-387-19557-5\n",
      "edit\n",
      "None\n",
      "Natural language processing\n",
      "v\n",
      "t\n",
      "e\n",
      "Natural language processing\n",
      "AI-complete\n",
      "Bag-of-words\n",
      "n-gram\n",
      "Bigram\n",
      "Trigram\n",
      "Computational linguistics\n",
      "Natural-language understanding\n",
      "Stop words\n",
      "Text processing\n",
      "Text analysis\n",
      "Collocation extraction\n",
      "Concept mining\n",
      "Coreference resolution\n",
      "Deep linguistic processing\n",
      "Distant reading\n",
      "Information extraction\n",
      "Named-entity recognition\n",
      "Ontology learning\n",
      "Parsing\n",
      "Part-of-speech tagging\n",
      "Semantic role labeling\n",
      "Semantic similarity\n",
      "Sentiment analysis\n",
      "Terminology extraction\n",
      "Text mining\n",
      "Textual entailment\n",
      "Truecasing\n",
      "Word-sense disambiguation\n",
      "Word-sense induction\n",
      "Text segmentation\n",
      "Compound-term processing\n",
      "Lemmatisation\n",
      "Lexical analysis\n",
      "Text chunking\n",
      "Stemming\n",
      "Sentence segmentation\n",
      "Word segmentation\n",
      "Automatic summarization\n",
      "Multi-document summarization\n",
      "Sentence extraction\n",
      "Text simplification\n",
      "Machine translation\n",
      "Computer-assisted\n",
      "Example-based\n",
      "Rule-based\n",
      "Statistical\n",
      "Transfer-based\n",
      "Neural\n",
      "Distributional semantics\n",
      "BERT\n",
      "Document-term matrix\n",
      "Explicit semantic analysis\n",
      "fastText\n",
      "GloVe\n",
      "Language model\n",
      "large\n",
      "Latent semantic analysis\n",
      "Seq2seq\n",
      "Word embedding\n",
      "Word2vec\n",
      "Language resources\n",
      "Corpus linguistics\n",
      "Lexical resource\n",
      "Linguistic Linked Open Data\n",
      "Machine-readable dictionary\n",
      "Parallel text\n",
      "PropBank\n",
      "Semantic network\n",
      "Simple Knowledge Organization System\n",
      "Speech corpus\n",
      "Text corpus\n",
      "Thesaurus (information retrieval)\n",
      "Treebank\n",
      "Universal Dependencies\n",
      "BabelNet\n",
      "Bank of English\n",
      "DBpedia\n",
      "FrameNet\n",
      "Google Ngram Viewer\n",
      "UBY\n",
      "WordNet\n",
      "None\n",
      "Speech recognition\n",
      "Speech segmentation\n",
      "Speech synthesis\n",
      "Natural language generation\n",
      "Optical character recognition\n",
      "Topic model\n",
      "Document classification\n",
      "Latent Dirichlet allocation\n",
      "Pachinko allocation\n",
      "None\n",
      "Automated essay scoring\n",
      "Concordancer\n",
      "Grammar checker\n",
      "Predictive text\n",
      "Pronunciation assessment\n",
      "Spell checker\n",
      "Syntax guessing\n",
      "None\n",
      "Chatbot\n",
      "Interactive fiction\n",
      "Question answering\n",
      "Virtual assistant\n",
      "Voice user interface\n",
      "Hallucination\n",
      "Natural Language Toolkit\n",
      "spaCy\n",
      "Portal\n",
      "None\n",
      "Language\n",
      "Authority control\n",
      "None\n",
      "Israel\n",
      "United States\n",
      "Japan\n",
      "Czech Republic\n",
      "https://en.wikipedia.org/w/index.php?title=Natural_language_processing&oldid=1166281924\n",
      "Categories\n",
      "Natural language processing\n",
      "Computational fields of study\n",
      "Computational linguistics\n",
      "Speech recognition\n",
      "All accuracy disputes\n",
      "Accuracy disputes from December 2013\n",
      "Harv and Sfn no-target errors\n",
      "CS1 maint: location\n",
      "Articles with short description\n",
      "Short description is different from Wikidata\n",
      "Commons category link from Wikidata\n",
      "Articles with J9U identifiers\n",
      "Articles with LCCN identifiers\n",
      "Articles with NDL identifiers\n",
      "Articles with NKC identifiers\n",
      "Creative Commons Attribution-ShareAlike License 4.0\n",
      "None\n",
      "Terms of Use\n",
      "Privacy Policy\n",
      "Wikimedia Foundation, Inc.\n",
      "Privacy policy\n",
      "About Wikipedia\n",
      "Disclaimers\n",
      "Contact Wikipedia\n",
      "Code of Conduct\n",
      "Mobile view\n",
      "Developers\n",
      "Statistics\n",
      "Cookie statement\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "for x in soup.find_all('a'): print(x.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8c12e2",
   "metadata": {},
   "source": [
    "Extracting all text of a particular tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec22affd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in soup.find_all('p'): print(x.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af97f2c5",
   "metadata": {},
   "source": [
    "### 1-6. Parsing Text Using Regular Expressions"
   ]
  },
  {
   "cell_type": "raw",
   "id": "80bde469",
   "metadata": {},
   "source": [
    "patterns\n",
    "throughout multiple lines.\n",
    "• re.S: This flag is used to find dot matches.\n",
    "• re.U: This flag is used to work for unicode data.\n",
    "• re.X: This flag is used for writing regex in a more\n",
    "readable format.\n",
    "Regular expressions’ functionality:\n",
    "• Find the single occurrence of character a and b:\n",
    "Regex: [ab]\n",
    "• Find characters except for a and b:\n",
    "Regex: [^ab]\n",
    "• Find the character range of a to z:\n",
    "Regex: [a-z]\n",
    "• Find a range except to z:\n",
    "Regex: [^a-z]\n",
    "• Find all the characters a to z as well as A to Z:\n",
    "Regex: [a-zA-Z]\n",
    "• Any single character:\n",
    "Regex:\n",
    "• Any whitespace character:\n",
    "Regex: \\s\n",
    "• Any non-whitespace character:\n",
    "Regex: \\S\n",
    "• Any digit:\n",
    "Regex: \\d\n",
    "• Any non-digit:\n",
    "Regex: \\D\n",
    "• Any non-words:\n",
    "Regex: \\W\n",
    "• Any words:\n",
    "Regex: \\w\n",
    "• Either match a or b:\n",
    "Regex: (a|b)\n",
    "• The occurrence of a is either zero or one:\n",
    "• Matches zero or one occurrence but not more than\n",
    "one occurrence\n",
    "Regex: a? ; ?\n",
    "• The occurrence of a is zero times or more than that:\n",
    "Regex: a* ; * matches zero or more than that\n",
    "• The occurrence of a is one time or more than that:\n",
    "Regex: a+ ; + matches occurrences one or more that\n",
    "one time\n",
    "• Exactly match three occurrences of a:\n",
    "Regex: a{3}\n",
    "• Match simultaneous occurrences of a with 3 or more\n",
    "than 3:\n",
    "Regex: a{3,}\n",
    "• Match simultaneous occurrences of a between 3 to 6:\n",
    "Regex: a{3,6}\n",
    "• Starting of the string:\n",
    "Regex: ^\n",
    "• Ending of the string:\n",
    "Regex: $\n",
    "• Match word boundary:\n",
    "Regex: \\b\n",
    "• Non-word boundary:\n",
    "Regex: \\B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a6d87d",
   "metadata": {},
   "source": [
    "re.match() and re.search() functions are used to find the patterns\n",
    "and then can be processed according to the requirements of the application.\n",
    "Let’s look at the differences between re.match() and re.search():\n",
    "1. re.match(): This checks for a match of the string only\n",
    "at the beginning of the string. So, if it finds the pattern\n",
    "at the beginning of the input string, then it returns the\n",
    "matched pattern; otherwise; it returns a noun.\n",
    "2. re.search(): This checks for a match of the string\n",
    "anywhere in the string. It finds all the occurrences of\n",
    "the pattern in the given input string or data.\n",
    "Now let’s look at a few of the examples using these regular expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9661f1",
   "metadata": {},
   "source": [
    "#### Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "9dfc7810",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'like', 'this', 'book.']"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import library\n",
    "import re\n",
    "#run the split query\n",
    "re.split('\\s+','I like this book.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "cefe751b",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 'Hi How are you'.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "facb047d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi', 'How', 'are', 'you']"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe76b25",
   "metadata": {},
   "source": [
    "#### Extracing email IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "51136523",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = \"For more details please mail us at: xyz@abc.com,pqr@mno.com,learnwithme4998@gmail.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "d6de0774",
   "metadata": {},
   "outputs": [],
   "source": [
    "addresses = re.findall(r'[\\w\\.-]+@[\\w\\.-]+', doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "998fe068",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xyz@abc.com', 'pqr@mno.com', 'learnwithme4998@gmail.com']"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "addresses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523b9608",
   "metadata": {},
   "source": [
    "#### Replacing email IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "c94ec9bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For more details please mail us at: learnwithme4998@gmail.com,learnwithme4998@gmail.com,learnwithme4998@gmail.com\n"
     ]
    }
   ],
   "source": [
    "new_email_address = re.sub(r'([\\w\\.-]+)@([\\w\\.-]+)',r'learnwithme4998@gmail.com', doc)\n",
    "print(new_email_address)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c040aaa",
   "metadata": {},
   "source": [
    "### 1-7. Handling Strings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205961cb",
   "metadata": {},
   "source": [
    "The simplest way to do this is by using the below string functionality.\n",
    "- s.find(t) index of first instance of string t inside s (-1 if not found)\n",
    "- s.rfind(t) index of last instance of string t inside s (-1 if not found)\n",
    "- s.index(t) like s.find(t) except it raises ValueError if not found\n",
    "- s.rindex(t) like s.rfind(t) except it raises ValueError if not found\n",
    "- s.join(text) combine the words of the text into a string\n",
    "- using s as the glue\n",
    "- s.split(t) split s into a list wherever a t is found\n",
    "- (whitespace by default)\n",
    "- s.splitlines() split s into a list of strings, one per line\n",
    "- s.lower() a lowercased version of the string s\n",
    "- s.upper() an uppercased version of the string s\n",
    "- s.title() a titlecased version of the string s\n",
    "- s.strip() a copy of s without leading or trailing whitespace\n",
    "- s.replace(t, u) replace instances of t with u inside s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "43048965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "exploring\n"
     ]
    }
   ],
   "source": [
    "String_v1 = \"I am exploring NLP\"\n",
    "#To extract particular character or range of characters from string\n",
    "print(String_v1[0])\n",
    "#To extract exploring\n",
    "print(String_v1[5:14])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aec8764",
   "metadata": {},
   "source": [
    "Replace “exploring” with “Learnwithme” in the above string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "c86fa257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am Learning NLP\n"
     ]
    }
   ],
   "source": [
    "String_v2 = String_v1.replace(\"exploring\", \"Learning\")\n",
    "print(String_v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ad84d2",
   "metadata": {},
   "source": [
    "#### Concatenating two strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "07248e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nlp machine learning\n"
     ]
    }
   ],
   "source": [
    "s1 = \"nlp\"\n",
    "s2 = \" machine learning\"\n",
    "s3 = s1+s2\n",
    "print(s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60087a3",
   "metadata": {},
   "source": [
    "#### Searching for a substring in a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "07fa1c92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var=\"I am learning NLP\"\n",
    "f= \"learn\"\n",
    "var.find(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27515f1c",
   "metadata": {},
   "source": [
    "### 1-8. Scraping Text from the Web"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc015b4",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">Caution Before scraping any websites, blogs, or e-commerce websites, please make sure you read the terms and conditions of the websites on whether it gives permissions for data scraping.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24798854",
   "metadata": {},
   "source": [
    "The simplest way to do this is by using beautiful soup or scrapy library\n",
    "from Python. Let’s use beautiful soup in this recipe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9477b4",
   "metadata": {},
   "source": [
    "#### Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "68e4d634",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "from ipywidgets import FloatProgress\n",
    "from time import sleep\n",
    "from IPython.display import display\n",
    "import re\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cf7bb6",
   "metadata": {},
   "source": [
    "#### Identify the url to extract the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "9dda26ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://www.imdb.com/chart/top?ref_=nv_mv_250_6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "4e0bb0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = requests.get(url)\n",
    "c = result.content\n",
    "soup = BeautifulSoup(c,\"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "e634ca45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<html>\n",
       "<head><title>403 Forbidden</title></head>\n",
       "<body>\n",
       "<center><h1>403 Forbidden</h1></center>\n",
       "</body>\n",
       "</html>"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "485fb3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Send a GET request to the IMDB Top Rated Movies page\n",
    "url = 'https://www.imdb.com/chart/top/'\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create a BeautifulSoup object from the response content\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find the movie titles and ratings using appropriate CSS selectors\n",
    "titles = soup.select('.lister-list tr .titleColumn a')\n",
    "ratings = soup.select('.lister-list tr .imdbRating strong')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "1f94313b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [403]>"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d381ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the text from the elements and print the movie details\n",
    "for title, rating in zip(titles, ratings):\n",
    "    movie_title = title.text.strip()\n",
    "    movie_rating = rating.text.strip()\n",
    "    print(f\"Title: {movie_title}, Rating: {movie_rating}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f60c5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef20140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading imdb top 250 movie's data\n",
    "url = 'http://www.imdb.com/chart/top'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa7d1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe729073",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "ab945c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "e02c81f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading imdb top 250 movie's data\n",
    "url = 'http://www.imdb.com/chart/top'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "5d6d6f0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [403]>"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "c85673fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = soup.select('td.titleColumn')\n",
    "crew = [a.attrs.get('title') for a in soup.select('td.titleColumn a')]\n",
    "ratings = [b.attrs.get('data-value')\n",
    "\t\tfor b in soup.select('td.posterColumn span[name=ir]')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "b18a64e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a empty list for storing\n",
    "# movie information\n",
    "list = []\n",
    "\n",
    "# Iterating over movies to extract\n",
    "# each movie's details\n",
    "for index in range(0, len(movies)):\n",
    "\t\n",
    "\t# Separating movie into: 'place',\n",
    "\t# 'title', 'year'\n",
    "\tmovie_string = movies[index].get_text()\n",
    "\tmovie = (' '.join(movie_string.split()).replace('.', ''))\n",
    "\tmovie_title = movie[len(str(index))+1:-7]\n",
    "\tyear = re.search('\\((.*?)\\)', movie_string).group(1)\n",
    "\tplace = movie[:len(str(index))-(len(movie))]\n",
    "\tdata = {\"place\": place,\n",
    "\t\t\t\"movie_title\": movie_title,\n",
    "\t\t\t\"rating\": ratings[index],\n",
    "\t\t\t\"year\": year,\n",
    "\t\t\t\"star_cast\": crew[index],\n",
    "\t\t\t}\n",
    "\tlist.append(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "25703f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "for movie in list:\n",
    "\tprint(movie['place'], '-', movie['movie_title'], '('+movie['year'] +\n",
    "\t\t') -', 'Starring:', movie['star_cast'], movie['rating'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "e632a4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the list as dataframe\n",
    "#then converting into .csv file\n",
    "df = pd.DataFrame(list)\n",
    "df.to_csv('imdb_top_250_movies.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "e1628224",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "87c523ac",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'urlencode' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-207-80ccd78fecba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mproxy_url\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_scrapeops_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'http://quotes.toscrape.com/page/1/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-207-80ccd78fecba>\u001b[0m in \u001b[0;36mget_scrapeops_url\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_scrapeops_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mpayload\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'api_key'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAPI_KEY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'url'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mproxy_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'https://proxy.scrapeops.io/v1/?'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0murlencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpayload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mproxy_url\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'urlencode' is not defined"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "API_KEY = '85748e0c-cf08-45fa-afdb-1f665eb2c13a'\n",
    "\n",
    "def get_scrapeops_url(url):\n",
    "    payload = {'api_key': API_KEY, 'url': url}\n",
    "    proxy_url = 'https://proxy.scrapeops.io/v1/?' + urlencode(payload)\n",
    "    return proxy_url\n",
    "\n",
    "r = requests.get(get_scrapeops_url('http://quotes.toscrape.com/page/1/'))\n",
    "print(r.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fae6df4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
