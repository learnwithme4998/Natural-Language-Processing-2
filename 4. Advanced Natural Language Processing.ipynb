{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ef1db42",
   "metadata": {},
   "source": [
    "In this Lecuture, we are going to cover various advanced NLP techniques\n",
    "and leverage machine learning algorithms to extract information from text\n",
    "data as well as some of the advanced NLP applications with the solution\n",
    "approach and implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8af93c0",
   "metadata": {},
   "source": [
    "1. Noun Phrase extraction\n",
    "2. Text similarity\n",
    "3. Parts of speech tagging\n",
    "4. Information extraction – NER – Entity recognition\n",
    "5. Topic modeling\n",
    "6. Text classification\n",
    "7. Sentiment analysis\n",
    "8. Word sense disambiguation\n",
    "9. Speech recognition and speech to text\n",
    "10. Text to speech\n",
    "11. Language detection and translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3f156c",
   "metadata": {},
   "source": [
    "Before getting into recipes, let’s understand the NLP pipeline and life\n",
    "cycle first. There are so many concepts we are implementing in this book,\n",
    "and we might get overwhelmed by the content of it. To make it simpler\n",
    "and smoother, let’s see what is the flow that we need to follow for an NLP\n",
    "solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4173e1c3",
   "metadata": {},
   "source": [
    "For example, let’s consider customer sentiment analysis and\n",
    "prediction for a product or brand or service.\n",
    "- **Define the Problem**: Understand the customer sentiment across the products.\n",
    "\n",
    "- **Understand the depth and breadth of the problem**: Understand the customer/user sentiments across the product; why we are doing this? What is the business impact? Etc.\n",
    "\n",
    "- **Data requirement brainstorming**: Have a brainstorming activity to list out all possible data points.\n",
    "  -All the reviews from customers on e-commerce\n",
    "    platforms like Amazon, Flipkart, etc.\n",
    " - Emails sent by customers\n",
    " - Warranty claim forms\n",
    " - Survey data\n",
    " - Call center conversations using speech to text\n",
    " - Feedback forms\n",
    " - Social media data like Twitter, Facebook, and LinkedIn\n",
    "\n",
    "- **Data collection**: We learned different techniques to collect the data in . Based on the data and the problem, we might have to incorporate different data collection methods. In this case, we can use web scraping and Twitter APIs.\n",
    "\n",
    "\n",
    "- **Text Preprocessing**: We know that data won’t always be clean. We need to spend a significant amount of time to process it and extract insight out of it using different methods that we discussed earlier in\n",
    "\n",
    "- **Text to feature**: As we discussed, texts are characters and machines will have a tough time understanding them. We have to convert them to features that machines and algorithms can understand using any of the methods we learned in the previous chapter.\n",
    "\n",
    "\n",
    "- **Machine learning/Deep learning**: Machine learning/Deep learning is a part of an artificial intelligence umbrella that will make systems automatically learn patterns in the data without being programmed. Most of the NLP solutions are based on this, and since we converted text to features, we can leverage machine learning or deep learning algorithms to achieve the goals like text classification, natural language generation, etc.\n",
    "\n",
    "\n",
    "- **Insights and deployment**: There is absolutely no use for building NLP solutions without proper insights being communicated to the business. Always take time to connect the dots between model/analysis output and the business, thereby creating the maximum impact."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29070e57",
   "metadata": {},
   "source": [
    "## 1. Extracting Noun Phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10560c27",
   "metadata": {},
   "source": [
    "Noun Phrase extraction is important when you want to analyze the “who”\n",
    "in a sentence. Let’s see an example below using TextBlob."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15849cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m textblob.download_corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6689bf2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "john\n",
      "natural language processing.he\n",
      "usa\n"
     ]
    }
   ],
   "source": [
    "#Import libraries\n",
    "import nltk\n",
    "from textblob import TextBlob\n",
    "#Extract noun\n",
    "blob = TextBlob(\"John is learning natural language processing.He lives in USA\")\n",
    "for np in blob.noun_phrases:\n",
    "    print(np)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8957ad40",
   "metadata": {},
   "source": [
    "## 2. Finding Similarity Between Texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddc18f5",
   "metadata": {},
   "source": [
    "we are going to discuss how to find the similarity between\n",
    "two documents or text. There are many similarity metrics like Euclidian,\n",
    "cosine, Jaccard, etc. Applications of text similarity can be found in areas\n",
    "like spelling correction and data deduplication.\n",
    "Here are a few of the similarity measures:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ad57c5",
   "metadata": {},
   "source": [
    "- **Cosine similarity**: Calculates the cosine of the angle between the two vectors.\n",
    "- **Jaccard similarity**: The score is calculated using the intersection or union of words.\n",
    "- **Jaccard Index** = (the number in both sets) / (the number in either set) * 100.\n",
    "- **Levenshtein distance**: Minimal number of insertions, deletions, and replacements required for transforming string “a” into string “b.”\n",
    "- **Hamming distance**: Number of positions with the same symbol in both strings. But it can be defined only for strings with equal length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32552b28",
   "metadata": {},
   "source": [
    "The simplest way to do this is by using cosine similarity from the sklearn\n",
    "library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee658b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = (\n",
    "\"I like NLP\",\n",
    "\"I am exploring ML\",\n",
    "\"I am a beginner in NLP\",\n",
    "\"I want to learn NLP\",\n",
    "\"I like advanced NLP\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600e3564",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d21f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e427de78",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a35135",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tfidf_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa4ab5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b35f25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix[0:1].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64401c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute similarity for first sentence with rest of the sentences\n",
    "cosine_similarity(tfidf_matrix[0:1],tfidf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164535e4",
   "metadata": {},
   "source": [
    "If we clearly observe, the first sentence and last sentence have higher\n",
    "similarity compared to the rest of the sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b20fe0",
   "metadata": {},
   "source": [
    "### Phonetic matching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3fbdee",
   "metadata": {},
   "source": [
    "The next version of similarity checking is phonetic matching, which roughly\n",
    "matches the two words or sentences and also creates an alphanumeric\n",
    "string as an encoded version of the text or word. It is very useful for searching\n",
    "large text corpora, correcting spelling errors, and matching relevant names.\n",
    "Soundex and Metaphone are two main phonetic algorithms used for this\n",
    "purpose. The simplest way to do this is by using the fuzzy library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eff2b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def soundex(word):\n",
    "    soundex_code = word[0].upper()\n",
    "\n",
    "    mapping = {\n",
    "        'BFPV': '1',\n",
    "        'CGJKQSXZ': '2',\n",
    "        'DT': '3',\n",
    "        'L': '4',\n",
    "        'MN': '5',\n",
    "        'R': '6'\n",
    "    }\n",
    "\n",
    "    for char in word[1:]:\n",
    "        for key, code in mapping.items():\n",
    "            if char.upper() in key:\n",
    "                if code != soundex_code[-1]:\n",
    "                    soundex_code += code\n",
    "\n",
    "    soundex_code = soundex_code.replace(\"0\", \"\")\n",
    "    soundex_code = soundex_code[:4].ljust(4, \"0\")\n",
    "    \n",
    "    return soundex_code\n",
    "\n",
    "# Example usage\n",
    "word = \"Smith\"\n",
    "print(\"Soundex code for '{}': {}\".format(word, soundex(word)))  # Output: S530\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051fb401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "word = \"Smythe\"\n",
    "print(\"Soundex code for '{}': {}\".format(word, soundex(word)))  # Output: S530"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281a9d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "word = \"John\"\n",
    "print(\"Soundex code for '{}': {}\".format(word, soundex(word)))  # Output: S530"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375edc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install metaphone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea99930f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from metaphone import doublemetaphone\n",
    "\n",
    "def fuzzy_soundex(word):\n",
    "    return doublemetaphone(word)\n",
    "\n",
    "# Example usage\n",
    "word = \"Smith\"\n",
    "print(\"Fuzzy Soundex code for '{}': {}\".format(word, fuzzy_soundex(word)))\n",
    "# Example usage\n",
    "word = \"Smythe\"\n",
    "print(\"Fuzzy Soundex code for '{}': {}\".format(word, fuzzy_soundex(word)))\n",
    "word = \"John\"\n",
    "print(\"Fuzzy Soundex code for '{}': {}\".format(word, fuzzy_soundex(word)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced0c302",
   "metadata": {},
   "source": [
    "## 3. Tagging Part of Speech"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3c335e",
   "metadata": {},
   "source": [
    "Part of speech (POS) tagging is another crucial part of natural language\n",
    "processing that involves labeling the words with a part of speech such as\n",
    "noun, verb, adjective, etc. POS is the base for Named Entity Resolution,\n",
    "Sentiment Analysis, Question Answering, and Word Sense Disambiguation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04702c03",
   "metadata": {},
   "source": [
    "There are 2 ways a tagger can be built.\n",
    "- **Rule based** - Rules created manually, which tag a word belonging to a particular POS.\n",
    "- **Stochastic based** - These algorithms capture the sequence of the words and tag the probability of the sequence using hidden Markov models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e541ff",
   "metadata": {},
   "source": [
    "Again, NLTK has the best POS tagging module. nltk.pos_tag(word) is the\n",
    "function that will generate the POS tagging for any given word. Use for loop\n",
    "and generate POS for all the words present in the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fb897a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Text = \"I love NLP and I will learn NLP in 2 month\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67ee727",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652a5371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary packages and stopwords\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# Tokenize the text\n",
    "tokens = sent_tokenize(Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1f301d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ac6bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d48ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate tagging for all the tokens using loop\n",
    "for i in tokens:\n",
    "    words = nltk.word_tokenize(i)\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "# POS-tagger.\n",
    "    tags = nltk.pos_tag(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e594c20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01dc7101",
   "metadata": {},
   "source": [
    "Below are the short forms and explanation of POS tagging. The word\n",
    "“love” is VBP, which means verb, sing. present, non-3d take."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4941ae38",
   "metadata": {},
   "source": [
    "- CC coordinating conjunction\n",
    "- CD cardinal digit\n",
    "- DT determiner\n",
    "- EX existential there (like: “there is” ... think of it like “there exists”)\n",
    "- FW foreign word\n",
    "- IN preposition/subordinating conjunction\n",
    "- JJ adjective ‘big’\n",
    "- JJR adjective, comparative ‘bigger’\n",
    "- JJS adjective, superlative ‘biggest’\n",
    "- LS list marker 1)\n",
    "- MD modal could, will\n",
    "- NN noun, singular ‘desk’\n",
    "- NNS noun plural ‘desks’\n",
    "- NNP proper noun, singular ‘Harrison’\n",
    "- NNPS proper noun, plural ‘Americans’\n",
    "- PDT predeterminer ‘all the kids’\n",
    "- POS possessive ending parent’s\n",
    "- PRP personal pronoun I, he, she\n",
    "- PRP possessive pronoun my, his, hers\n",
    "- RB adverb very, silently\n",
    "- RBR adverb, comparative better\n",
    "- RBS adverb, superlative best\n",
    "- RP particle give up\n",
    "- TO to go ‘to’ the store\n",
    "- UH interjection\n",
    "- VB verb, base form take\n",
    "- VBD verb, past tense took\n",
    "- VBG verb, gerund/present participle taking\n",
    "- VBN verb, past participle taken\n",
    "- VBP verb, sing. present, non-3d take\n",
    "- VBZ verb, 3rd person sing. present takes\n",
    "- WDT wh-determiner which\n",
    "- WP wh-pronoun who, what\n",
    "- WP possessive wh-pronoun whose\n",
    "- WRB wh-adverb where, when"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13207087",
   "metadata": {},
   "source": [
    "## 4. Extract Entities from Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386a7641",
   "metadata": {},
   "source": [
    "Named Entity Recognition (NER) is a natural language processing (NLP) technique used to identify and classify named entities in text into predefined categories such as person names, organization names, locations, dates, and more. The goal of NER is to extract and understand the key entities mentioned in the text, providing a deeper level of understanding about the content.\n",
    "\n",
    "Named entities are specific pieces of information that refer to real-world objects and entities. For example, in the sentence \"Barack Obama was born in Hawaii and served as the 44th President of the United States,\" the named entities are \"Barack Obama\" (PERSON), \"Hawaii\" (GPE - geopolitical entity), \"the 44th President\" (ORDINAL), and \"the United States\" (GPE).\n",
    "\n",
    "NER is essential in various NLP applications such as information extraction, question answering, sentiment analysis, and text summarization. It helps in structuring unstructured text data by identifying relevant entities and their types.\n",
    "\n",
    "How NER works:\n",
    "1. Tokenization: The input text is split into individual words or tokens.\n",
    "2. Part-of-Speech (POS) Tagging: Each token is assigned a part-of-speech tag, such as noun, verb, adjective, etc.\n",
    "3. Chunking: Groups of tokens with specific POS tags are identified, forming chunks or phrases.\n",
    "4. Entity Recognition: Using context and patterns, the algorithm identifies and classifies chunks into named entity categories like PERSON, ORGANIZATION, GPE, DATE, etc.\n",
    "\n",
    "NER models are often built using machine learning techniques, particularly sequence labeling approaches like conditional random fields (CRFs) or deep learning architectures like bidirectional LSTM (Long Short-Term Memory) networks.\n",
    "\n",
    "Libraries like `spaCy`, NLTK, StanfordNLP, and Flair offer NER capabilities and make it easier to perform Named Entity Recognition in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a10c432",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"John is studying at ,,  Stanford University in California\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb24834b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a925852",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ghostscript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9173012",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import nltk\n",
    "from nltk import ne_chunk\n",
    "from nltk import word_tokenize\n",
    "#NER\n",
    "ne_chunk(nltk.pos_tag(word_tokenize(sent)), binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27413ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenize(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f98442",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb2f8cf",
   "metadata": {},
   "source": [
    "### Using SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15efd626",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "# Read/create a sentence\n",
    "doc = nlp(u'Apple is ready to launch new phone worth $10000 in New york time square ')\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e0f95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88785b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Example text\n",
    "text = \"Apple Inc. is a technology company based in California, United States. It was founded by Steve Jobs and Steve Wozniak on April 1, 1976.\"\n",
    "\n",
    "# Process the text using SpaCy's NLP pipeline\n",
    "doc = nlp(text)\n",
    "\n",
    "# Extract and display named entities\n",
    "for entity in doc.ents:\n",
    "    print(entity.text, entity.label_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d083999e",
   "metadata": {},
   "source": [
    "According to the output, Apple is an organization, 10000 is money, and\n",
    "New York is place. The results are accurate and can be used for any NLP\n",
    "applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a2de69",
   "metadata": {},
   "source": [
    "## 5. Extracting Topics from Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488a20aa",
   "metadata": {},
   "source": [
    "We are going to discuss how to identify topics from the\n",
    "document. Say, for example, there is an online library with multiple\n",
    "departments based on the kind of book. As the new book comes in,\n",
    "you want to look at the unique keywords/topics and decide on which\n",
    "department this book might belong to and place it accordingly. In these\n",
    "kinds of situations, topic modeling would be handy.\n",
    "Basically, this is document tagging and clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a645e6c9",
   "metadata": {},
   "source": [
    "### 5.1 Create the text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff18f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = \"I am learning NLP, it is very interesting and exciting.it includes machine learning and deep learning\"\n",
    "doc2 = \"My father is a data scientist and he is nlp expert\"\n",
    "doc3 = \"My sister has good exposure into android development\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0995282f",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_complete = [doc1, doc2, doc3]\n",
    "doc_complete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f377a20c",
   "metadata": {},
   "source": [
    "### 5.2 Cleaning and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281a5188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install and import libraries\n",
    "!pip install gensim\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d6d687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584b2051",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stop)\n",
    "print(exclude)\n",
    "print(lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78796f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()\n",
    "def clean(doc):   \n",
    "    # Lowercase the document and split it into words\n",
    "    words = doc.lower().split()\n",
    "    \n",
    "    # Remove stopwords and punctuation marks, and lemmatize the words\n",
    "    cleaned_words = [lemma.lemmatize(word) for word in words if word not in stop and word not in exclude]\n",
    "    \n",
    "    # Join the cleaned words back into a single string\n",
    "    return \" \".join(cleaned_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a2299b",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6500a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean(doc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37851bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma.lemmatize('learning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41ed050",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953e66c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and normalize each sentence using the 'clean' function\n",
    "cleaned_sentences = [clean(sentence).split() for sentence in doc_complete]\n",
    "\n",
    "print(cleaned_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8ce485",
   "metadata": {},
   "source": [
    "### 5-3 Preparing document term matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef436688",
   "metadata": {},
   "source": [
    "A Document-Term Matrix (DTM) is a widely used representation of text data in Natural Language Processing (NLP) and Information Retrieval (IR). It is a mathematical matrix that describes the frequency of terms (words or phrases) that occur in a collection of documents. Each row of the matrix represents a document, and each column represents a term. The elements of the matrix are typically the term frequencies or some other measure of importance for the terms within the corresponding documents.\n",
    "\n",
    "The DTM is a fundamental data structure used in various text analysis tasks, including text classification, topic modeling, sentiment analysis, and more. It serves as the basis for many text-based machine learning models.\n",
    "\n",
    "Here's how a Document-Term Matrix is constructed:\n",
    "\n",
    "1. Tokenization: The text in each document is first tokenized into individual words or terms. Tokenization breaks the text into smaller units, typically words, but it can also be n-grams (combinations of n words).\n",
    "\n",
    "2. Vocabulary Creation: The vocabulary is the set of all unique terms that appear in the collection of documents. Each term becomes a column in the DTM.\n",
    "\n",
    "3. Counting Term Frequencies: For each document, the occurrence of each term is counted. The count represents how many times a specific term appears in that document.\n",
    "\n",
    "4. Assembling the Matrix: The DTM is constructed by arranging the term frequencies for each document in a matrix, where rows represent documents, and columns represent terms.\n",
    "\n",
    "Here's a simple example of a Document-Term Matrix:\n",
    "\n",
    "Consider the following three sentences:\n",
    "\n",
    "1. \"I love natural language processing.\"\n",
    "2. \"Text analysis is fascinating.\"\n",
    "3. \"Natural language processing is a subfield of NLP.\"\n",
    "\n",
    "Vocabulary: love, natural, language, processing, text, analysis, fascinating, is, a, subfield, of, NLP.\n",
    "\n",
    "DTM:\n",
    "\n",
    "| Document | love | natural | language | processing | text | analysis | fascinating | is | a | subfield | of | NLP |\n",
    "|----------|------|---------|----------|------------|------|----------|-------------|----|---|---------|----|-----|\n",
    "| Sentence 1 | 1    | 1        | 1         | 1           | 0    | 0        | 0            | 1   | 0 | 0       | 0  | 0   |\n",
    "| Sentence 2 | 0    | 0        | 0         | 0           | 1    | 1        | 1            | 1   | 0 | 0       | 0  | 0   |\n",
    "| Sentence 3 | 0    | 1        | 1         | 1           | 0    | 0        | 0            | 1   | 1 | 1       | 1  | 2   |\n",
    "\n",
    "In this example, the DTM represents the three sentences and the term frequencies for each term within each sentence. The DTM is a useful data representation for further text analysis and modeling tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4acb5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing gensim\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "# Creating the term dictionary of our corpus, where every unique term is assigned an index.\n",
    "dictionary = corpora.Dictionary(cleaned_sentences)\n",
    "# Converting a list of documents (corpus) into Document-TermMatrix using dictionary prepared above.\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in cleaned_sentences]\n",
    "print(doc_term_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c236226d",
   "metadata": {},
   "source": [
    "### 5.4 LDA model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae6dede",
   "metadata": {},
   "source": [
    "Latent Dirichlet Allocation (LDA) is a widely used probabilistic generative model in Natural Language Processing (NLP) and Machine Learning for topic modeling. LDA is used to discover hidden topics within a collection of documents by analyzing the distribution of words across these topics. It is an unsupervised learning algorithm that can automatically identify the main themes or topics present in the text data.\n",
    "\n",
    "The LDA model assumes that each document is a mixture of various topics, and each topic is a probability distribution over words. The goal of LDA is to infer the topic distribution for each document and the word distribution for each topic.\n",
    "\n",
    "Here's a high-level overview of how the LDA model works:\n",
    "\n",
    "1. Initialization: Choose the number of topics `k` you want to discover in the corpus of documents.\n",
    "\n",
    "2. Preprocessing: Prepare the text data by tokenizing the documents, removing stop words, and performing other text cleaning steps.\n",
    "\n",
    "3. Create the Document-Term Matrix (DTM): Represent the text data as a DTM, where rows represent documents, and columns represent terms (words).\n",
    "\n",
    "4. LDA Model Fitting: Apply the LDA algorithm to the DTM to find the topic distributions for each document and the word distributions for each topic. The algorithm uses probabilistic inference to estimate these distributions.\n",
    "\n",
    "5. Interpretation: Examine the output of the LDA model to understand the discovered topics. Each topic will have a distribution of words, and each document will have a distribution of topics. The most probable words in each topic can be considered the representative words of that topic.\n",
    "\n",
    "6. Topic Assignments: The LDA model assigns a probability distribution of topics to each document. It allows documents to be associated with multiple topics to varying degrees.\n",
    "\n",
    "Applications of LDA:\n",
    "- Topic Modeling: Discovering the main themes or topics within a collection of documents.\n",
    "- Document Clustering: Grouping similar documents based on their topic distributions.\n",
    "- Information Retrieval: Enhancing search results by associating documents with relevant topics.\n",
    "- Recommender Systems: Providing recommendations based on user preferences and document-topic associations.\n",
    "\n",
    "LDA is a powerful tool for unsupervised analysis of text data and is widely used in various domains to gain insights from large collections of documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9dc825a",
   "metadata": {},
   "source": [
    "Let's walk through an example of using the LDA model for topic modeling with sample texts. For simplicity, we'll use a smaller set of documents.\n",
    "\n",
    "Sample Texts:\n",
    "1. \"I love natural language processing.\"\n",
    "2. \"Text analysis is fascinating.\"\n",
    "3. \"Natural language processing is a subfield of NLP.\"\n",
    "4. \"Machine learning is an essential part of NLP.\"\n",
    "5. \"NLP applications are widespread in the industry.\"\n",
    "\n",
    "Now, let's use the LDA model to discover topics in these sample texts:\n",
    "\n",
    "Step 1: Preprocessing\n",
    "We'll preprocess the text data by tokenizing, converting to lowercase, and removing stopwords. We'll also remove single-character words and punctuations.\n",
    "\n",
    "Preprocessed Texts:\n",
    "1. [\"love\", \"natural\", \"language\", \"processing\"]\n",
    "2. [\"text\", \"analysis\", \"fascinating\"]\n",
    "3. [\"natural\", \"language\", \"processing\", \"subfield\", \"NLP\"]\n",
    "4. [\"machine\", \"learning\", \"essential\", \"part\", \"NLP\"]\n",
    "5. [\"NLP\", \"applications\", \"widespread\", \"industry\"]\n",
    "\n",
    "Step 2: Create Document-Term Matrix (DTM)\n",
    "The DTM will represent the text data, where each row corresponds to a document, and each column corresponds to a term (word).\n",
    "\n",
    "|            | love | natural | language | processing | text | analysis | fascinating | subfield | NLP | machine | learning | essential | part | applications | widespread | industry |\n",
    "|------------|------|---------|----------|------------|------|----------|-------------|----------|-----|---------|----------|-----------|------|--------------|-----------|----------|\n",
    "| Document 1 | 1    | 1       | 1        | 0          | 0    | 0        | 0           | 0        | 0   | 0       | 0        | 0         | 0    | 0            | 0         | 0        |\n",
    "| Document 2 | 0    | 0       | 0        | 1          | 1    | 1        | 0           | 0        | 0   | 0       | 0        | 0         | 0    | 0            | 0         | 0        |\n",
    "| Document 3 | 0    | 1       | 1        | 0          | 0    | 1        | 1           | 0        | 0   | 0       | 0        | 0         | 0    | 0            | 0         | 0        |\n",
    "| Document 4 | 0    | 0       | 0        | 0          | 0    | 0        | 0           | 1        | 1   | 1       | 0        | 0         | 0    | 0            | 0         | 0        |\n",
    "| Document 5 | 0    | 0       | 1        | 0          | 0    | 0        | 0           | 0        | 1   | 0       | 0        | 1         | 1    | 1            | 1         | 1        |\n",
    "\n",
    "\n",
    "Step 3: Train the LDA Model\n",
    "We'll use the DTM to train the LDA model and discover topics in the sample texts.\n",
    "\n",
    "Step 4: Interpretation\n",
    "The LDA model will provide us with topic distributions for each document and word distributions for each topic. We can then interpret the topics based on the most probable words in each topic.\n",
    "\n",
    "In this example, the LDA model may discover topics like:\n",
    "- Topic 1: \"NLP, natural language processing, subfield\"\n",
    "- Topic 2: \"Text analysis, fascinating\"\n",
    "- Topic 3: \"Machine learning, essential part\"\n",
    "- Topic 4: \"Applications, widespread, industry\"\n",
    "\n",
    "Keep in mind that the number of topics and their interpretations depend on the input data and the hyperparameters chosen for the LDA model. The goal of the LDA model is to identify topics that capture the main themes present in the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd96e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the object for LDA model using gensim library\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "# Running and Training LDA model on the document term matrixfor 3 topics.\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=3, id2word =dictionary, passes=50)\n",
    "# Results\n",
    "print(ldamodel.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c22def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the LDA model\n",
    "num_topics = 2  # Number of topics to discover\n",
    "lda_model = gensim.models.LdaModel(doc_term_matrix, num_topics=num_topics, id2word=dictionary, passes=10)\n",
    "\n",
    "# Print the discovered topics and their most probable words\n",
    "for topic_id in range(num_topics):\n",
    "    print(f\"Topic {topic_id + 1}: {lda_model.print_topic(topic_id)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928f5d3b",
   "metadata": {},
   "source": [
    "All the weights associated with the topics from the sentence seem\n",
    "almost similar. You can perform this on huge data to extract significant\n",
    "topics. The whole idea to implement this on sample data is to make you\n",
    "familiar with it, and you can use the same code snippet to perform on the\n",
    "huge data for significant results and insights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadb25ce",
   "metadata": {},
   "source": [
    "## 6. Classifying Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fe2267",
   "metadata": {},
   "source": [
    "Text classification – The aim of text classification is to automatically classifythe text documents based on pretrained categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11d9a8a",
   "metadata": {},
   "source": [
    "Applications:\n",
    "- Sentiment Analysis\n",
    "- Document classification\n",
    "- Spam – ham mail classification\n",
    "- Resume shortlisting\n",
    "- Document summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e2dd6f",
   "metadata": {},
   "source": [
    "Spam - ham classification using machine learning\n",
    "\"Spam\" and \"ham\" are terms commonly used in the context of email classification and spam filtering. They refer to two categories of emails based on their content.\n",
    "\n",
    "1. Spam:\n",
    "Spam refers to unsolicited or unwanted emails, typically sent in bulk to a large number of recipients. These emails often contain promotional content, advertisements, scams, or other forms of irrelevant and potentially harmful messages. The primary purpose of sending spam emails is usually to promote products, services, or fraudulent activities.\n",
    "\n",
    "2. Ham:\n",
    "Ham, on the other hand, refers to legitimate and non-spam emails. These are the desired emails that users expect to receive from known and trusted sources, such as personal or work-related emails, newsletters from subscribed services, and communication from friends, colleagues, or organizations.\n",
    "\n",
    "Spam filtering is a technique used to automatically identify and separate spam emails from legitimate ones (ham) in an email inbox. Machine learning algorithms, such as Naive Bayes, Support Vector Machines, and Deep Learning models, are commonly used for spam filtering tasks. These algorithms analyze the content and characteristics of emails to classify them as either spam or ham.\n",
    "\n",
    "By using spam filters, email providers and users can help reduce the annoyance and potential risks associated with receiving unwanted or harmful spam emails, ensuring that legitimate emails reach the inbox while unwanted spam is filtered out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01eeb18",
   "metadata": {},
   "source": [
    "If you observe, your Gmail has a folder called “Spam.” It will basically\n",
    "classify your emails into spam and ham so that you don’t have to read\n",
    "unnecessary emails."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b767f4",
   "metadata": {},
   "source": [
    "### 6.1 Data collection and understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a422e224",
   "metadata": {},
   "source": [
    "Please download data from the below link and save it in your working\n",
    "directory:<br>\n",
    "\n",
    "https://www.kaggle.com/uciml/sms-spam-collection-dataset#spam.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c01c2208",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['v1', 'v2', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], dtype='object')"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#Read the data\n",
    "Email_Data = pd.read_csv(\"spam.csv\",encoding ='latin1')\n",
    "#Data undestanding\n",
    "Email_Data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c9d05ee2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will Ì_ b going to esplanade fr home?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        v1                                                 v2 Unnamed: 2  \\\n",
       "0      ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
       "1      ham                      Ok lar... Joking wif u oni...        NaN   \n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
       "3      ham  U dun say so early hor... U c already then say...        NaN   \n",
       "4      ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
       "...    ...                                                ...        ...   \n",
       "5567  spam  This is the 2nd time we have tried 2 contact u...        NaN   \n",
       "5568   ham              Will Ì_ b going to esplanade fr home?        NaN   \n",
       "5569   ham  Pity, * was in mood for that. So...any other s...        NaN   \n",
       "5570   ham  The guy did some bitching but I acted like i'd...        NaN   \n",
       "5571   ham                         Rofl. Its true to its name        NaN   \n",
       "\n",
       "     Unnamed: 3 Unnamed: 4  \n",
       "0           NaN        NaN  \n",
       "1           NaN        NaN  \n",
       "2           NaN        NaN  \n",
       "3           NaN        NaN  \n",
       "4           NaN        NaN  \n",
       "...         ...        ...  \n",
       "5567        NaN        NaN  \n",
       "5568        NaN        NaN  \n",
       "5569        NaN        NaN  \n",
       "5570        NaN        NaN  \n",
       "5571        NaN        NaN  \n",
       "\n",
       "[5572 rows x 5 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Email_Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9711e9be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>Email</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Target                                              Email\n",
       "0    ham  Go until jurong point, crazy.. Available only ...\n",
       "1    ham                      Ok lar... Joking wif u oni...\n",
       "2   spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3    ham  U dun say so early hor... U c already then say...\n",
       "4    ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Email_Data = Email_Data[['v1', 'v2']]\n",
    "Email_Data = Email_Data.rename(columns={\"v1\":\"Target\",\"v2\":\"Email\"})\n",
    "Email_Data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172da9d4",
   "metadata": {},
   "source": [
    "### 6.2 Text processing and feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0a621761",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from textblob import TextBlob\n",
    "from nltk.stem import PorterStemmer\n",
    "from textblob import Word\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "import sklearn.feature_extraction.text as text\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "525b6800",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre processing steps like lower case, stemming and lemmatization\n",
    "st = PorterStemmer()\n",
    "Email_Data['Email'] = Email_Data['Email'].apply(lambda x:\" \".join(x.lower() for x in x.split()))\n",
    "stop = stopwords.words('english')\n",
    "Email_Data['Email'] = Email_Data['Email'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "Email_Data['Email'] = Email_Data['Email'].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))\n",
    "Email_Data['Email'] = Email_Data['Email'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "78bf6f17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'u dun say earli hor... u c alreadi say...'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Email_Data['Email'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3b1a5308",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting data into train and validation\n",
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(Email_Data['Email'], Email_Data['Target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7654bcad",
   "metadata": {},
   "source": [
    " ### 6.3 Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa8f48b",
   "metadata": {},
   "source": [
    "In the context of TF-IDF vectorization, the terms \"analyzer\" and \"token pattern\" are important concepts that relate to how text is processed and tokenized before creating the TF-IDF representation.\n",
    "\n",
    "1. Analyzer:\n",
    "The analyzer is a component in TF-IDF vectorization that defines how the text data should be preprocessed and tokenized before generating the TF-IDF representation. It essentially determines the unit of analysis, which can be at the word level, character level, or any other custom level.\n",
    "\n",
    "The default analyzer in scikit-learn's TfidfVectorizer is `word`, which means that the input text will be tokenized into individual words. For example, the sentence \"I love natural language processing\" will be tokenized into the following individual words: [\"I\", \"love\", \"natural\", \"language\", \"processing\"].\n",
    "\n",
    "You can also specify a custom analyzer by providing your own function. For instance, you could create a custom analyzer that tokenizes the text at the character level, or applies additional preprocessing steps like stemming or lemmatization before tokenization.\n",
    "\n",
    "2. Token Pattern:\n",
    "The token pattern is a regular expression (regex) that specifies what constitutes a \"token\" during tokenization. It defines the rules for breaking the text into smaller units, which are then considered as individual tokens.\n",
    "\n",
    "By default, the token pattern in scikit-learn's TfidfVectorizer is `r\"(?u)\\b\\w\\w+\\b\"`. This pattern means that tokens are formed by sequences of two or more word characters (letters, digits, or underscores) bounded by word boundaries. The `(?u)` is used to indicate that Unicode word boundaries should be used, allowing tokenization of words with non-ASCII characters.\n",
    "\n",
    "For example, with the default token pattern, the sentence \"Text analysis is fascinating\" will be tokenized into the following words: [\"Text\", \"analysis\", \"is\", \"fascinating\"].\n",
    "\n",
    "You can customize the token pattern to suit your specific requirements. For instance, if you want to include single-letter words, you can modify the pattern to `r\"(?u)\\b\\w+\\b\"`. If you want to consider phrases as tokens, you can use a different regex pattern to match phrases based on your desired criteria.\n",
    "\n",
    "In summary, the analyzer and token pattern in TF-IDF vectorization allow you to control how text data is preprocessed, tokenized, and converted into a numerical representation. These options are essential for fine-tuning the TF-IDF vectorization process to best suit your specific text analysis needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a77abce8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.57107987, 0.30126137, 0.35392139, ..., 0.27726381, 0.20726274,\n",
       "       0.30688117])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TFIDF feature generation for a maximum of 5000 features\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word',token_pattern=r'\\w{1,}', max_features=5000)\n",
    "tfidf_vect.fit(Email_Data['Email'])\n",
    "xtrain_tfidf = tfidf_vect.transform(train_x)\n",
    "xvalid_tfidf = tfidf_vect.transform(valid_x)\n",
    "xtrain_tfidf.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bafb5cb",
   "metadata": {},
   "source": [
    "### 6.3 Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c40d44b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(classifier, feature_vector_train, label,feature_vector_valid, is_neural_net=False):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    return metrics.accuracy_score(predictions, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e1a7cfca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9877961234745154\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes trainig\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(alpha=0.2),xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print (\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c1dab071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9655419956927495\n"
     ]
    }
   ],
   "source": [
    "# Linear Classifier on Word Level TF IDF Vectors\n",
    "accuracy = train_model(linear_model.LogisticRegression(),xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print (\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f57b01",
   "metadata": {},
   "source": [
    "Naive Bayes is giving better results than the linear classifier. We can try\n",
    "many more classifiers and then choose the best one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3557147b",
   "metadata": {},
   "source": [
    "## 7. Carrying Out Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51fc97a",
   "metadata": {},
   "source": [
    "We are going to discuss how to understand the sentiment of\n",
    "a particular sentence or statement. Sentiment analysis is one of the widely\n",
    "used techniques across the industries to understand the sentiments of the\n",
    "customers/users around the products/services. Sentiment analysis gives\n",
    "the sentiment score of a sentence/statement tending toward positive or\n",
    "negative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982e6e26",
   "metadata": {},
   "source": [
    "The simplest way to do this by using a **TextBlob** or **vedar** library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd66bef",
   "metadata": {},
   "source": [
    "Let’s follow the steps in this section to do sentiment analysis using\n",
    "TextBlob. It will basically give 2 metrics.\n",
    "- Polarity = Polarity lies in the range of [-1,1] where 1 means a positive statement and -1 means a negative statement.\n",
    "- Subjectivity = Subjectivity refers that mostly it is a public opinion and not factual information [0,1].\n",
    "\n",
    "Polarity and subjectivity are two important concepts in sentiment analysis and text analysis:\n",
    "\n",
    "1. Polarity:\n",
    "Polarity refers to the sentiment expressed in a piece of text, indicating whether the text expresses a positive, negative, or neutral sentiment. In sentiment analysis, the polarity score typically ranges from -1 to 1, where:\n",
    "   - A polarity score of -1 indicates a highly negative sentiment.\n",
    "   - A polarity score of 0 indicates a neutral sentiment.\n",
    "   - A polarity score of 1 indicates a highly positive sentiment.\n",
    "\n",
    "For example:\n",
    "- \"I love this product! It's amazing.\" - Positive polarity (polarity score close to 1)\n",
    "- \"I hate Mondays.\" - Negative polarity (polarity score close to -1)\n",
    "- \"The weather is fine.\" - Neutral polarity (polarity score close to 0)\n",
    "\n",
    "2. Subjectivity:\n",
    "Subjectivity refers to the degree of subjectiveness or objectiveness in a piece of text. A subjective statement expresses personal opinions, emotions, or feelings, while an objective statement presents factual information without any personal bias. Subjectivity is usually measured on a scale from 0 to 1, where:\n",
    "   - A subjectivity score of 0 indicates complete objectivity (the text is purely factual).\n",
    "   - A subjectivity score of 1 indicates complete subjectivity (the text is purely opinionated or emotional).\n",
    "\n",
    "For example:\n",
    "- \"The sun rises in the east.\" - Objective (subjectivity score close to 0)\n",
    "- \"In my opinion, the movie was fantastic!\" - Subjective (subjectivity score close to 1)\n",
    "\n",
    "In sentiment analysis, both polarity and subjectivity scores are commonly used to understand the sentiment expressed in the text and to distinguish between objective and subjective statements. These scores are helpful for various NLP applications, including sentiment analysis, text classification, and opinion mining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b1b3b40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "review = \"I like this phone. screen quality and camera clarity is really good.\"\n",
    "review2 = \"This tv is not good. Bad quality, no clarity, worste xperience\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7effb1f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=-0.5249999999999999, subjectivity=0.6333333333333333)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import libraries\n",
    "from textblob import TextBlob\n",
    "#TextBlob has a pre trained sentiment prediction model\n",
    "blob = TextBlob(review2)\n",
    "blob.sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69223e1",
   "metadata": {},
   "source": [
    "This is a negative review, as the polarity is “-0.52.”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d673c464",
   "metadata": {},
   "source": [
    "## 8. Disambiguating Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79526832",
   "metadata": {},
   "source": [
    "There is ambiguity that arises due to a different meaning of words in a\n",
    "different context.\n",
    "For example,\n",
    "\n",
    "- Text1 = 'I went to the bank to deposit my money'\n",
    "- Text2 = 'The river bank was full of dead fishes'\n",
    "\n",
    "In the above texts, the word “bank” has different meanings based on\n",
    "the context of the sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecc3a22",
   "metadata": {},
   "source": [
    "The Lesk algorithm is one of the best algorithms for word sense\n",
    "disambiguation. Let’s see how to solve using the package pywsd and nltk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a9ea32",
   "metadata": {},
   "source": [
    "The Lesk Algorithm is a word sense disambiguation (WSD) algorithm used to determine the correct sense of an ambiguous word within a given context. It was proposed by Michael E. Lesk in 1986. Word sense disambiguation is an important task in natural language processing (NLP) to resolve the ambiguity that arises when a word has multiple meanings or senses depending on its context.\n",
    "\n",
    "The Lesk Algorithm is based on the idea that words in a given context tend to share similar words in their definitions. It utilizes the glosses (short definitions) of different word senses from a lexical database, such as WordNet, and compares these glosses with the words in the context of the ambiguous word.\n",
    "\n",
    "Here's a high-level overview of the Lesk Algorithm:\n",
    "\n",
    "1. Given an ambiguous word and its context sentence.\n",
    "2. Retrieve the glosses (short definitions) of all possible word senses of the ambiguous word from a lexical database like WordNet.\n",
    "3. Tokenize the context sentence to obtain individual words.\n",
    "4. For each word in the context sentence, calculate the overlap between the glosses of each word sense and the set of words in the context. The overlap is determined using set intersection or other similarity metrics.\n",
    "5. Choose the word sense with the highest overlap as the disambiguated sense for the ambiguous word.\n",
    "\n",
    "By comparing the glosses of different word senses with the words in the context, the Lesk Algorithm aims to find the most relevant sense that best fits the context.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The Lesk Algorithm has been widely used in NLP tasks, especially in the field of lexical semantics and word sense disambiguation. While the algorithm is relatively simple, it provides a practical and effective approach for resolving word sense ambiguity in various applications, such as machine translation, information retrieval, and question answering systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c856d665",
   "metadata": {},
   "source": [
    "Step-by-step pictorial representation of the Lesk Algorithm with a simple example to help you visualize the process.\n",
    "\n",
    "Let's consider the word \"bank,\" which can have multiple senses (meanings) depending on the context:\n",
    "\n",
    "1. Bank (financial institution)\n",
    "2. Bank (side of a river)\n",
    "\n",
    "Context Sentence: \"I went to the bank to deposit some money.\"\n",
    "\n",
    "- Step 1: Get Synsets from WordNet\n",
    "Retrieve all possible word senses (synsets) of the word \"bank\" from WordNet.\n",
    "\n",
    "- Step 2: Get Glosses (Short Definitions) Retrieve the glosses (short definitions) for each synset.\n",
    "\n",
    "```\n",
    "Synset 1 (Bank - financial institution):\n",
    "    Gloss: \"a financial establishment that uses money deposited by customers for investment, pays it out when required, makes loans at interest, and exchanges currency.\"\n",
    "\n",
    "Synset 2 (Bank - side of a river):\n",
    "    Gloss: \"sloping land (especially the slope beside a body of water).\"\n",
    "```\n",
    "\n",
    "- Step 3: Tokenize the Context Sentence Tokenize the words in the context sentence to obtain individual words.\n",
    "\n",
    "Context Tokens: [\"I\", \"went\", \"to\", \"the\", \"bank\", \"to\", \"deposit\", \"some\", \"money\"]\n",
    "\n",
    "- Step 4: Calculate Overlap For each synset's gloss, calculate the overlap with the words in the context sentence. The overlap can be determined using set intersection or other similarity metrics.\n",
    "\n",
    "```\n",
    "Overlap with Synset 1:\n",
    "    \"bank\" and \"financial\" - 1 word match\n",
    "    \"bank\" and \"institution\" - 1 word match\n",
    "    Total overlap: 2\n",
    "\n",
    "Overlap with Synset 2:\n",
    "    \"bank\" and \"sloping\" - 0 word match\n",
    "    \"bank\" and \"land\" - 1 word match\n",
    "    \"bank\" and \"side\" - 1 word match\n",
    "    \"bank\" and \"river\" - 0 word match\n",
    "    Total overlap: 2\n",
    "```\n",
    "\n",
    "- Step 5: Choose the Best Sense\n",
    "Select the synset with the highest overlap as the disambiguated sense for the ambiguous word \"bank.\" In this example, both Synset 1 and Synset 2 have the same overlap (2 words), so we may need further context or disambiguation techniques to make the final decision.\n",
    "\n",
    "In more complex examples, the Lesk Algorithm considers more words and senses, and the overlap calculations become more extensive. However, this simplified example illustrates the core idea of how the Lesk Algorithm works to disambiguate an ambiguous word based on its context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2516dea0",
   "metadata": {},
   "source": [
    "### 8.1 Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4ec9289c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pywsd in /Users/jothiramsanjeevi/opt/anaconda3/lib/python3.8/site-packages (1.2.5)\n",
      "Requirement already satisfied: nltk in /Users/jothiramsanjeevi/opt/anaconda3/lib/python3.8/site-packages (from pywsd) (3.6.1)\n",
      "Requirement already satisfied: numpy in /Users/jothiramsanjeevi/opt/anaconda3/lib/python3.8/site-packages (from pywsd) (1.20.1)\n",
      "Requirement already satisfied: pandas in /Users/jothiramsanjeevi/opt/anaconda3/lib/python3.8/site-packages (from pywsd) (1.2.4)\n",
      "Requirement already satisfied: wn==0.0.23 in /Users/jothiramsanjeevi/opt/anaconda3/lib/python3.8/site-packages (from pywsd) (0.0.23)\n",
      "Requirement already satisfied: six in /Users/jothiramsanjeevi/opt/anaconda3/lib/python3.8/site-packages (from pywsd) (1.15.0)\n",
      "Requirement already satisfied: click in /Users/jothiramsanjeevi/opt/anaconda3/lib/python3.8/site-packages (from nltk->pywsd) (7.1.2)\n",
      "Requirement already satisfied: joblib in /Users/jothiramsanjeevi/opt/anaconda3/lib/python3.8/site-packages (from nltk->pywsd) (1.0.1)\n",
      "Requirement already satisfied: regex in /Users/jothiramsanjeevi/opt/anaconda3/lib/python3.8/site-packages (from nltk->pywsd) (2021.4.4)\n",
      "Requirement already satisfied: tqdm in /Users/jothiramsanjeevi/opt/anaconda3/lib/python3.8/site-packages (from nltk->pywsd) (4.59.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/jothiramsanjeevi/opt/anaconda3/lib/python3.8/site-packages (from pandas->pywsd) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /Users/jothiramsanjeevi/opt/anaconda3/lib/python3.8/site-packages (from pandas->pywsd) (2021.1)\n",
      "\u001b[33mDEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warming up PyWSD (takes ~10 secs)... took 7.8118109703063965 secs.\n"
     ]
    }
   ],
   "source": [
    "#First, import the libraries:\n",
    "#Install pywsd\n",
    "!pip install pywsd\n",
    "#Import functions\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import PorterStemmer\n",
    "from itertools import chain\n",
    "from pywsd.lesk import simple_lesk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58860c8e",
   "metadata": {},
   "source": [
    "### 8.2 Disambiguating word sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6e2c819a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context-1: I went to the bank to deposit my money\n",
      "Sense: Synset('depository_financial_institution.n.01')\n",
      "Definition :  a financial institution that accepts deposits and channels the money into lending activities\n",
      "Context-2: The river bank was full of dead fishes\n",
      "Sense: Synset('bank.n.01')\n",
      "Definition :  sloping land (especially the slope beside a body of water)\n"
     ]
    }
   ],
   "source": [
    "# Sentences\n",
    "bank_sents = ['I went to the bank to deposit my money','The river bank was full of dead fishes']\n",
    "# calling the lesk function and printing results for both thesentences\n",
    "print (\"Context-1:\", bank_sents[0])\n",
    "answer = simple_lesk(bank_sents[0],'bank')\n",
    "print (\"Sense:\", answer)\n",
    "print (\"Definition : \", answer.definition())\n",
    "print (\"Context-2:\", bank_sents[1])\n",
    "answer = simple_lesk(bank_sents[1],'bank','n')\n",
    "print (\"Sense:\", answer)\n",
    "print (\"Definition : \", answer.definition())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a73ca0",
   "metadata": {},
   "source": [
    "Observe that in context-1, “bank” is a financial institution, but in\n",
    "context-2, “bank” is sloping land"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4507c72f",
   "metadata": {},
   "source": [
    "## 9. Converting Speech to Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafd1af4",
   "metadata": {},
   "source": [
    "The simplest way to do this by using Speech Recognition and PyAudio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e60123",
   "metadata": {},
   "source": [
    "Interaction with machines is trending toward the voice, which is the usual\n",
    "way of human communication. Popular examples are Siri, Alexa’s Google\n",
    "Voice, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d382ce63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: SpeechRecognition in /Users/jothiramsanjeevi/opt/anaconda3/lib/python3.8/site-packages (3.10.0)\n",
      "Requirement already satisfied: requests>=2.26.0 in /Users/jothiramsanjeevi/opt/anaconda3/lib/python3.8/site-packages (from SpeechRecognition) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/jothiramsanjeevi/opt/anaconda3/lib/python3.8/site-packages (from requests>=2.26.0->SpeechRecognition) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/jothiramsanjeevi/opt/anaconda3/lib/python3.8/site-packages (from requests>=2.26.0->SpeechRecognition) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/jothiramsanjeevi/opt/anaconda3/lib/python3.8/site-packages (from requests>=2.26.0->SpeechRecognition) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jothiramsanjeevi/opt/anaconda3/lib/python3.8/site-packages (from requests>=2.26.0->SpeechRecognition) (2020.12.5)\n",
      "\u001b[33mDEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: PyAudio in /Users/jothiramsanjeevi/opt/anaconda3/lib/python3.8/site-packages (0.2.11)\n",
      "\u001b[33mDEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install SpeechRecognition\n",
    "!pip install PyAudio\n",
    "import speech_recognition as sr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1222f5dd",
   "metadata": {},
   "source": [
    "If you are using Mac machine you may face problem while installing the Pyaudio use the below code in the terminal to install the package"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602789f6",
   "metadata": {},
   "source": [
    "Now after you run the below code snippet, whatever you say on the\n",
    "microphone (using recognize_google function) gets converted into text.\n",
    "\n",
    "<span style=\"color:red;\">conda install -c anaconda pyaudio</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2509c685",
   "metadata": {},
   "outputs": [],
   "source": [
    "r=sr.Recognizer()\n",
    "with sr.Microphone() as source:\n",
    "    print(\"Please say something\")\n",
    "    audio = r.listen(source)\n",
    "    print(\"Time over, thanks\")\n",
    "try:\n",
    "    print(\"I think you said: \"+r.recognize_google(audio));\n",
    "except:\n",
    "    pass;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2446c36",
   "metadata": {},
   "source": [
    "## 10. Converting Text to Speech"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723fe4e3",
   "metadata": {},
   "source": [
    "Converting text to speech is another useful NLP technique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0f5f6d",
   "metadata": {},
   "source": [
    "The simplest way to do this by using the gTTs library.\n",
    "\n",
    "\n",
    "The \"gTTS\" (Google Text-to-Speech) library is a Python library and CLI tool that allows you to convert text into speech using Google Text-to-Speech API. It provides a simple interface to generate speech from text, and it supports multiple languages and various voice options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "df3ee8ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gTTS in /Users/jothiramsanjeevi/opt/anaconda3/lib/python3.8/site-packages (2.3.2)\n",
      "Requirement already satisfied: requests<3,>=2.27 in /Users/jothiramsanjeevi/opt/anaconda3/lib/python3.8/site-packages (from gTTS) (2.31.0)\n",
      "Requirement already satisfied: click<8.2,>=7.1 in /Users/jothiramsanjeevi/opt/anaconda3/lib/python3.8/site-packages (from gTTS) (7.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/jothiramsanjeevi/opt/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.27->gTTS) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/jothiramsanjeevi/opt/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.27->gTTS) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/jothiramsanjeevi/opt/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.27->gTTS) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jothiramsanjeevi/opt/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.27->gTTS) (2020.12.5)\n",
      "\u001b[33mDEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install gTTS\n",
    "from gtts import gTTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130750d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273448e8",
   "metadata": {},
   "source": [
    "Now after you run the below code snippet, whatever you input in the text parameter gets converted into audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c08f2b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gtts import gTTS\n",
    "import os\n",
    "\n",
    "# Text to convert to speech\n",
    "text = \"LWM tamil youtube channel is the best in the world!\"\n",
    "# Create a gTTS object\n",
    "tts = gTTS(text=text, lang='en')\n",
    "\n",
    "# Save the speech as an audio file\n",
    "tts.save(\"output.mp3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462aa5ad",
   "metadata": {},
   "source": [
    "## 11. Translating Speech"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae4511b",
   "metadata": {},
   "source": [
    "Whenever you try to analyze data from blogs that are hosted across the\n",
    "globe, especially websites from countries like China, where Chinese is\n",
    "used predominantly, analyzing such data or performing NLP tasks on such\n",
    "data would be difficult. That’s where language translation comes to the rescue. You want to translate one language to another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "32ffca2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: goslate in /Users/jothiramsanjeevi/opt/anaconda3/lib/python3.8/site-packages (1.5.4)\n",
      "Requirement already satisfied: futures in /Users/jothiramsanjeevi/opt/anaconda3/lib/python3.8/site-packages (from goslate) (3.0.5)\n",
      "\u001b[33mDEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install goslate\n",
    "import goslate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "af0d52a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"LWM ist das beste YouTube Kanal der Welt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "1ab2c15b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LWM is the best YouTube channel in the world\n"
     ]
    }
   ],
   "source": [
    "gs = goslate.Goslate()\n",
    "translatedText = gs.translate(text,'en')\n",
    "print(translatedText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3183d28f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
