{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ef1db42",
   "metadata": {},
   "source": [
    "In this Lecuture, we are going to cover various advanced NLP techniques\n",
    "and leverage machine learning algorithms to extract information from text\n",
    "data as well as some of the advanced NLP applications with the solution\n",
    "approach and implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8af93c0",
   "metadata": {},
   "source": [
    "1. Noun Phrase extraction\n",
    "2. Text similarity\n",
    "3. Parts of speech tagging\n",
    "4. Information extraction – NER – Entity recognition\n",
    "5. Topic modeling\n",
    "6. Text classification\n",
    "7. Sentiment analysis\n",
    "8. Word sense disambiguation\n",
    "9. Speech recognition and speech to text\n",
    "10. Text to speech\n",
    "11. Language detection and translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3f156c",
   "metadata": {},
   "source": [
    "Before getting into recipes, let’s understand the NLP pipeline and life\n",
    "cycle first. There are so many concepts we are implementing in this book,\n",
    "and we might get overwhelmed by the content of it. To make it simpler\n",
    "and smoother, let’s see what is the flow that we need to follow for an NLP\n",
    "solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4173e1c3",
   "metadata": {},
   "source": [
    "For example, let’s consider customer sentiment analysis and\n",
    "prediction for a product or brand or service.\n",
    "- **Define the Problem**: Understand the customer sentiment across the products.\n",
    "\n",
    "- **Understand the depth and breadth of the problem**: Understand the customer/user sentiments across the product; why we are doing this? What is the business impact? Etc.\n",
    "\n",
    "- **Data requirement brainstorming**: Have a brainstorming activity to list out all possible data points.\n",
    "  -All the reviews from customers on e-commerce\n",
    "    platforms like Amazon, Flipkart, etc.\n",
    " - Emails sent by customers\n",
    " - Warranty claim forms\n",
    " - Survey data\n",
    " - Call center conversations using speech to text\n",
    " - Feedback forms\n",
    " - Social media data like Twitter, Facebook, and LinkedIn\n",
    "\n",
    "- **Data collection**: We learned different techniques to collect the data in Chapter 1. Based on the data and the problem, we might have to incorporate different data collection methods. In this case, we can use web scraping and Twitter APIs.\n",
    "\n",
    "\n",
    "- **Text Preprocessing**: We know that data won’t always be clean. We need to spend a significant amount of time to process it and extract insight out of it using different methods that we discussed earlier in\n",
    "\n",
    "- **Text to feature**: As we discussed, texts are characters and machines will have a tough time understanding them. We have to convert them to features that machines and algorithms can understand using any of the methods we learned in the previous chapter.\n",
    "\n",
    "\n",
    "- **Machine learning/Deep learning**: Machine learning/Deep learning is a part of an artificial intelligence umbrella that will make systems automatically learn patterns in the data without being programmed. Most of the NLP solutions are based on this, and since we converted text to features, we can leverage machine learning or deep learning algorithms to achieve the goals like text classification, natural language generation, etc.\n",
    "\n",
    "\n",
    "- **Insights and deployment**: There is absolutely no use for building NLP solutions without proper insights being communicated to the business. Always take time to connect the dots between model/analysis output and the business, thereby creating the maximum impact."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29070e57",
   "metadata": {},
   "source": [
    "## 1. Extracting Noun Phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10560c27",
   "metadata": {},
   "source": [
    "Noun Phrase extraction is important when you want to analyze the “who”\n",
    "in a sentence. Let’s see an example below using TextBlob."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d15849cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     /Users/jothiramsanjeevi/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/brown.zip.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/jothiramsanjeevi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/jothiramsanjeevi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/jothiramsanjeevi/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package conll2000 to\n",
      "[nltk_data]     /Users/jothiramsanjeevi/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/conll2000.zip.\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /Users/jothiramsanjeevi/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "!python -m textblob.download_corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6689bf2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "john\n",
      "natural language processing\n"
     ]
    }
   ],
   "source": [
    "#Import libraries\n",
    "import nltk\n",
    "from textblob import TextBlob\n",
    "#Extract noun\n",
    "blob = TextBlob(\"John is learning natural language processing\")\n",
    "for np in blob.noun_phrases:\n",
    "    print(np)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8957ad40",
   "metadata": {},
   "source": [
    "## 2. Finding Similarity Between Texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddc18f5",
   "metadata": {},
   "source": [
    "we are going to discuss how to find the similarity between\n",
    "two documents or text. There are many similarity metrics like Euclidian,\n",
    "cosine, Jaccard, etc. Applications of text similarity can be found in areas\n",
    "like spelling correction and data deduplication.\n",
    "Here are a few of the similarity measures:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ad57c5",
   "metadata": {},
   "source": [
    "- **Cosine similarity**: Calculates the cosine of the angle between the two vectors.\n",
    "- **Jaccard similarity**: The score is calculated using the intersection or union of words.\n",
    "- **Jaccard Index** = (the number in both sets) / (the number in either set) * 100.\n",
    "- **Levenshtein distance**: Minimal number of insertions, deletions, and replacements required for transforming string “a” into string “b.”\n",
    "- **Hamming distance**: Number of positions with the same symbol in both strings. But it can be defined only for strings with equal length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32552b28",
   "metadata": {},
   "source": [
    "The simplest way to do this is by using cosine similarity from the sklearn\n",
    "library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee658b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = (\n",
    "\"I like NLP\",\n",
    "\"I am exploring NLP\",\n",
    "\"I am a beginner in NLP\",\n",
    "\"I want to learn NLP\",\n",
    "\"I like advanced NLP\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "600e3564",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1d21f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "46a35135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.861037   0.50854232 0.         0.        ]\n",
      " [0.         0.58873218 0.         0.72971837 0.         0.\n",
      "  0.         0.34771471 0.         0.        ]\n",
      " [0.         0.4755751  0.58946308 0.         0.58946308 0.\n",
      "  0.         0.28088232 0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.55666851\n",
      "  0.         0.26525553 0.55666851 0.55666851]\n",
      " [0.72971837 0.         0.         0.         0.         0.\n",
      "  0.58873218 0.34771471 0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1aa4ab5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 10)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3b35f25a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.861037  , 0.50854232, 0.        , 0.        ]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_matrix[0:1].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a64401c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.17682765, 0.14284054, 0.13489366, 0.68374784]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#compute similarity for first sentence with rest of the sentences\n",
    "cosine_similarity(tfidf_matrix[0:1],tfidf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164535e4",
   "metadata": {},
   "source": [
    "If we clearly observe, the first sentence and last sentence have higher\n",
    "similarity compared to the rest of the sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b20fe0",
   "metadata": {},
   "source": [
    "### Phonetic matching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3fbdee",
   "metadata": {},
   "source": [
    "The next version of similarity checking is phonetic matching, which roughly\n",
    "matches the two words or sentences and also creates an alphanumeric\n",
    "string as an encoded version of the text or word. It is very useful for searching\n",
    "large text corpora, correcting spelling errors, and matching relevant names.\n",
    "Soundex and Metaphone are two main phonetic algorithms used for this\n",
    "purpose. The simplest way to do this is by using the fuzzy library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "70e0f69b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fuzzy in /Users/jothiramsanjeevi/opt/anaconda3/lib/python3.8/site-packages (1.2.2)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install fuzzy\n",
    "import fuzzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2584fde7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'FuzzySoundex' from 'fuzzy' (/Users/jothiramsanjeevi/opt/anaconda3/lib/python3.8/site-packages/fuzzy.cpython-38-darwin.so)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-b76a397725ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mfuzzy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFuzzySoundex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfuzzy_soundex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFuzzySoundex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'FuzzySoundex' from 'fuzzy' (/Users/jothiramsanjeevi/opt/anaconda3/lib/python3.8/site-packages/fuzzy.cpython-38-darwin.so)"
     ]
    }
   ],
   "source": [
    "from fuzzy import FuzzySoundex\n",
    "\n",
    "def fuzzy_soundex(word):\n",
    "    fs = FuzzySoundex()\n",
    "    return fs(word)\n",
    "\n",
    "# Example usage\n",
    "word = \"Smith\"\n",
    "print(\"Fuzzy Soundex code for '{}': {}\".format(word, fuzzy_soundex(word)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "07fdc3bb",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'ascii' codec can't decode byte 0xc0 in position 1: ordinal not in range(128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-a62719d56b6f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msoundex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'hi?'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32msrc/fuzzy.pyx\u001b[0m in \u001b[0;36mfuzzy.Soundex.__call__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'ascii' codec can't decode byte 0xc0 in position 1: ordinal not in range(128)"
     ]
    }
   ],
   "source": [
    "soundex('hi?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1eff2b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soundex code for 'Smith': S530\n"
     ]
    }
   ],
   "source": [
    "def soundex(word):\n",
    "    soundex_code = word[0].upper()\n",
    "\n",
    "    mapping = {\n",
    "        'BFPV': '1',\n",
    "        'CGJKQSXZ': '2',\n",
    "        'DT': '3',\n",
    "        'L': '4',\n",
    "        'MN': '5',\n",
    "        'R': '6'\n",
    "    }\n",
    "\n",
    "    for char in word[1:]:\n",
    "        for key, code in mapping.items():\n",
    "            if char.upper() in key:\n",
    "                if code != soundex_code[-1]:\n",
    "                    soundex_code += code\n",
    "\n",
    "    soundex_code = soundex_code.replace(\"0\", \"\")\n",
    "    soundex_code = soundex_code[:4].ljust(4, \"0\")\n",
    "    \n",
    "    return soundex_code\n",
    "\n",
    "# Example usage\n",
    "word = \"Smith\"\n",
    "print(\"Soundex code for '{}': {}\".format(word, soundex(word)))  # Output: S530\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "051fb401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soundex code for 'Natural': N364\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "word = \"Smythe\"\n",
    "print(\"Soundex code for '{}': {}\".format(word, soundex(word)))  # Output: S530"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "281a9d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soundex code for 'Naaatural': N364\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "word = \"SmNaithe\"\n",
    "print(\"Soundex code for '{}': {}\".format(word, soundex(word)))  # Output: S530"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "375edc30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting metaphone\n",
      "  Downloading Metaphone-0.6.tar.gz (14 kB)\n",
      "Building wheels for collected packages: metaphone\n",
      "  Building wheel for metaphone (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for metaphone: filename=Metaphone-0.6-py3-none-any.whl size=13905 sha256=8f2701898157912b17d5a4f7a40d30b1143504b7f57f495020fa35489c55acec\n",
      "  Stored in directory: /Users/jothiramsanjeevi/Library/Caches/pip/wheels/ea/4e/3b/0634f08a7980461ad630fa4147cbd25913b008b4a02992ba66\n",
      "Successfully built metaphone\n",
      "Installing collected packages: metaphone\n",
      "Successfully installed metaphone-0.6\n"
     ]
    }
   ],
   "source": [
    "!pip install metaphone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ea99930f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fuzzy Soundex code for 'Smith': ('SM0', 'XMT')\n"
     ]
    }
   ],
   "source": [
    "from metaphone import doublemetaphone\n",
    "\n",
    "def fuzzy_soundex(word):\n",
    "    return doublemetaphone(word)\n",
    "\n",
    "# Example usage\n",
    "word = \"Smith\"\n",
    "print(\"Fuzzy Soundex code for '{}': {}\".format(word, fuzzy_soundex(word)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced0c302",
   "metadata": {},
   "source": [
    "## 3. Tagging Part of Speech"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3c335e",
   "metadata": {},
   "source": [
    "Part of speech (POS) tagging is another crucial part of natural language\n",
    "processing that involves labeling the words with a part of speech such as\n",
    "noun, verb, adjective, etc. POS is the base for Named Entity Resolution,\n",
    "Sentiment Analysis, Question Answering, and Word Sense Disambiguation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04702c03",
   "metadata": {},
   "source": [
    "There are 2 ways a tagger can be built.\n",
    "- **Rule based** - Rules created manually, which tag a word belonging to a particular POS.\n",
    "- **Stochastic based** - These algorithms capture the sequence of the words and tag the probability of the sequence using hidden Markov models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e541ff",
   "metadata": {},
   "source": [
    "Again, NLTK has the best POS tagging module. nltk.pos_tag(word) is the\n",
    "function that will generate the POS tagging for any given word. Use for loop\n",
    "and generate POS for all the words present in the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "03fb897a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Text = \"I love NLP and I will learn NLP in 2 month\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d67ee727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "652a5371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary packages and stopwords\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# Tokenize the text\n",
    "tokens = sent_tokenize(Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1d1f301d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I love NLP and I will learn NLP in 2 month']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b1ac6bc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d6d48ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate tagging for all the tokens using loop\n",
    "for i in tokens:\n",
    "    words = nltk.word_tokenize(i)\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "# POS-tagger.\n",
    "    tags = nltk.pos_tag(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e594c20a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'),\n",
       " ('love', 'VBP'),\n",
       " ('NLP', 'NNP'),\n",
       " ('I', 'PRP'),\n",
       " ('learn', 'VBP'),\n",
       " ('NLP', 'RB'),\n",
       " ('2', 'CD'),\n",
       " ('month', 'NN')]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01dc7101",
   "metadata": {},
   "source": [
    "Below are the short forms and explanation of POS tagging. The word\n",
    "“love” is VBP, which means verb, sing. present, non-3d take."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4941ae38",
   "metadata": {},
   "source": [
    "- CC coordinating conjunction\n",
    "- CD cardinal digit\n",
    "- DT determiner\n",
    "- EX existential there (like: “there is” ... think of it like “there exists”)\n",
    "- FW foreign word\n",
    "- IN preposition/subordinating conjunction\n",
    "- JJ adjective ‘big’\n",
    "- JJR adjective, comparative ‘bigger’\n",
    "- JJS adjective, superlative ‘biggest’\n",
    "- LS list marker 1)\n",
    "- MD modal could, will\n",
    "- NN noun, singular ‘desk’\n",
    "- NNS noun plural ‘desks’\n",
    "- NNP proper noun, singular ‘Harrison’\n",
    "- NNPS proper noun, plural ‘Americans’\n",
    "- PDT predeterminer ‘all the kids’\n",
    "- POS possessive ending parent’s\n",
    "- PRP personal pronoun I, he, she\n",
    "- PRP possessive pronoun my, his, hers\n",
    "- RB adverb very, silently\n",
    "- RBR adverb, comparative better\n",
    "- RBS adverb, superlative best\n",
    "- RP particle give up\n",
    "- TO to go ‘to’ the store\n",
    "- UH interjection\n",
    "- VB verb, base form take\n",
    "- VBD verb, past tense took\n",
    "- VBG verb, gerund/present participle taking\n",
    "- VBN verb, past participle taken\n",
    "- VBP verb, sing. present, non-3d take\n",
    "- VBZ verb, 3rd person sing. present takes\n",
    "- WDT wh-determiner which\n",
    "- WP wh-pronoun who, what\n",
    "- WP possessive wh-pronoun whose\n",
    "- WRB wh-adverb where, when"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13207087",
   "metadata": {},
   "source": [
    "## 4. Extract Entities from Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386a7641",
   "metadata": {},
   "source": [
    "We are going to discuss how to identify and extract entities\n",
    "from the text, called Named Entity Recognition. There are multiple\n",
    "libraries to perform this task like NLTK chunker, StanfordNER, SpaCy,\n",
    "opennlp, and NeuroNER; and there are a lot of APIs also like WatsonNLU,\n",
    "AlchemyAPI, NERD, Google Cloud NLP API, and many more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3a10c432",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"John is studying at ,,  Stanford University in California\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cb24834b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/jothiramsanjeevi/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/jothiramsanjeevi/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9a925852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ghostscript\n",
      "  Downloading ghostscript-0.7-py2.py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: setuptools>=38.6.0 in /Users/jothiramsanjeevi/opt/anaconda3/lib/python3.8/site-packages (from ghostscript) (52.0.0.post20210125)\n",
      "Installing collected packages: ghostscript\n",
      "Successfully installed ghostscript-0.7\n"
     ]
    }
   ],
   "source": [
    "!pip install ghostscript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c9173012",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The Ghostscript executable isn't found.\n",
      "See http://web.mit.edu/ghostscript/www/Install.htm\n",
      "If you're using a Mac, you can try installing\n",
      "https://docs.brew.sh/Installation then `brew install ghostscript`\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/nltk/tree.py\u001b[0m in \u001b[0;36m_repr_png_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    817\u001b[0m                     [\n\u001b[0;32m--> 818\u001b[0;31m                         find_binary(\n\u001b[0m\u001b[1;32m    819\u001b[0m                             \u001b[0;34m\"gs\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/nltk/internals.py\u001b[0m in \u001b[0;36mfind_binary\u001b[0;34m(name, path_to_bin, env_vars, searchpath, binary_names, url, verbose)\u001b[0m\n\u001b[1;32m    687\u001b[0m ):\n\u001b[0;32m--> 688\u001b[0;31m     return next(\n\u001b[0m\u001b[1;32m    689\u001b[0m         find_binary_iter(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/nltk/internals.py\u001b[0m in \u001b[0;36mfind_binary_iter\u001b[0;34m(name, path_to_bin, env_vars, searchpath, binary_names, url, verbose)\u001b[0m\n\u001b[1;32m    672\u001b[0m     \"\"\"\n\u001b[0;32m--> 673\u001b[0;31m     for file in find_file_iter(\n\u001b[0m\u001b[1;32m    674\u001b[0m         \u001b[0mpath_to_bin\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msearchpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/nltk/internals.py\u001b[0m in \u001b[0;36mfind_file_iter\u001b[0;34m(filename, env_vars, searchpath, file_names, url, verbose, finding_dir)\u001b[0m\n\u001b[1;32m    631\u001b[0m         \u001b[0mdiv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"=\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m75\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 632\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\\n%s\\n%s\\n%s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdiv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n\n===========================================================================\nNLTK was unable to find the gs file!\nUse software specific configuration paramaters or set the PATH environment variable.\n===========================================================================",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/nltk/tree.py\u001b[0m in \u001b[0;36m_repr_png_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    835\u001b[0m                 )\n\u001b[1;32m    836\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpre_error_message\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 837\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    838\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "Tree('S', [Tree('PERSON', [('John', 'NNP')]), ('is', 'VBZ'), ('studying', 'VBG'), ('at', 'IN'), (',', ','), (',', ','), Tree('ORGANIZATION', [('Stanford', 'NNP'), ('University', 'NNP')]), ('in', 'IN'), Tree('GPE', [('California', 'NNP')])])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import libraries\n",
    "import nltk\n",
    "from nltk import ne_chunk\n",
    "from nltk import word_tokenize\n",
    "#NER\n",
    "ne_chunk(nltk.pos_tag(word_tokenize(sent)), binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d27413ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['John',\n",
       " 'is',\n",
       " 'studying',\n",
       " 'at',\n",
       " ',',\n",
       " ',',\n",
       " 'Stanford',\n",
       " 'University',\n",
       " 'in',\n",
       " 'California']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c8f98442",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['John',\n",
       " 'is',\n",
       " 'studying',\n",
       " 'at',\n",
       " ',,',\n",
       " 'Stanford',\n",
       " 'University',\n",
       " 'in',\n",
       " 'California']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb2f8cf",
   "metadata": {},
   "source": [
    "### Using SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "15efd626",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-71499a7eac27>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en_core_web_sm'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# Read/create a sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'Apple is ready to launch new phone worth $10000 in New york time square '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ment\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ments\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mRETURNS\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mLanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mloaded\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \"\"\"\n\u001b[0;32m---> 51\u001b[0;31m     return util.load_model(\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE941\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[index]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 472\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "# Read/create a sentence\n",
    "doc = nlp(u'Apple is ready to launch new phone worth $10000 in New york time square ')\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "74e0f95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.6.0-cp38-cp38-macosx_10_9_x86_64.whl (6.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.8 MB 2.4 MB/s eta 0:00:01     |██████████████▏                 | 3.0 MB 2.2 MB/s eta 0:00:02     |███████████████████████▉        | 5.1 MB 951 kB/s eta 0:00:02\n",
      "\u001b[?25hCollecting wasabi<1.2.0,>=0.9.1\n",
      "  Downloading wasabi-1.1.2-py3-none-any.whl (27 kB)\n",
      "Collecting typer<0.10.0,>=0.3.0\n",
      "  Downloading typer-0.9.0-py3-none-any.whl (45 kB)\n",
      "\u001b[K     |████████████████████████████████| 45 kB 2.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting smart-open<7.0.0,>=5.2.1\n",
      "  Downloading smart_open-6.3.0-py3-none-any.whl (56 kB)\n",
      "\u001b[K     |████████████████████████████████| 56 kB 2.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting thinc<8.2.0,>=8.1.8\n",
      "  Downloading thinc-8.1.10-cp38-cp38-macosx_10_9_x86_64.whl (850 kB)\n",
      "\u001b[K     |████████████████████████████████| 850 kB 2.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/jothiramsanjeevi/opt/anaconda3/lib/python3.8/site-packages (from spacy) (4.59.0)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Downloading spacy_loggers-1.0.4-py3-none-any.whl (11 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.7-cp38-cp38-macosx_10_9_x86_64.whl (32 kB)\n",
      "Collecting pathy>=0.10.0\n",
      "  Downloading pathy-0.10.2-py3-none-any.whl (48 kB)\n",
      "\u001b[K     |████████████████████████████████| 48 kB 2.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /Users/jothiramsanjeevi/opt/anaconda3/lib/python3.8/site-packages (from spacy) (2.11.3)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.9-cp38-cp38-macosx_10_9_x86_64.whl (18 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3\n",
      "  Downloading srsly-2.4.7-cp38-cp38-macosx_10_9_x86_64.whl (490 kB)\n",
      "\u001b[K     |████████████████████████████████| 490 kB 1.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.15.0 in /Users/jothiramsanjeevi/opt/anaconda3/lib/python3.8/site-packages (from spacy) (1.20.1)\n",
      "Requirement already satisfied: setuptools in /Users/jothiramsanjeevi/opt/anaconda3/lib/python3.8/site-packages (from spacy) (52.0.0.post20210125)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "\u001b[K     |████████████████████████████████| 181 kB 2.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.8-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/jothiramsanjeevi/opt/anaconda3/lib/python3.8/site-packages (from spacy) (2.31.0)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.8-cp38-cp38-macosx_10_9_x86_64.whl (107 kB)\n",
      "\u001b[K     |████████████████████████████████| 107 kB 550 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4\n",
      "  Downloading pydantic-1.10.11-cp38-cp38-macosx_10_9_x86_64.whl (2.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9 MB 1.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /Users/jothiramsanjeevi/opt/anaconda3/lib/python3.8/site-packages (from spacy) (20.9)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/jothiramsanjeevi/opt/anaconda3/lib/python3.8/site-packages (from packaging>=20.0->spacy) (2.4.7)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/jothiramsanjeevi/opt/anaconda3/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.7.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/jothiramsanjeevi/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/jothiramsanjeevi/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/jothiramsanjeevi/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jothiramsanjeevi/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.12.5)\n",
      "Collecting blis<0.8.0,>=0.7.8\n",
      "  Downloading blis-0.7.10-cp38-cp38-macosx_10_9_x86_64.whl (6.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.1 MB 1.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting confection<1.0.0,>=0.0.1\n",
      "  Downloading confection-0.1.0-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/jothiramsanjeevi/opt/anaconda3/lib/python3.8/site-packages (from typer<0.10.0,>=0.3.0->spacy) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/jothiramsanjeevi/opt/anaconda3/lib/python3.8/site-packages (from jinja2->spacy) (1.1.1)\n",
      "Installing collected packages: catalogue, srsly, pydantic, murmurhash, cymem, wasabi, typer, smart-open, preshed, confection, blis, thinc, spacy-loggers, spacy-legacy, pathy, langcodes, spacy\n",
      "  Attempting uninstall: smart-open\n",
      "    Found existing installation: smart-open 5.1.0\n",
      "    Uninstalling smart-open-5.1.0:\n",
      "      Successfully uninstalled smart-open-5.1.0\n",
      "Successfully installed blis-0.7.10 catalogue-2.0.8 confection-0.1.0 cymem-2.0.7 langcodes-3.3.0 murmurhash-1.0.9 pathy-0.10.2 preshed-3.0.8 pydantic-1.10.11 smart-open-6.3.0 spacy-3.6.0 spacy-legacy-3.0.12 spacy-loggers-1.0.4 srsly-2.4.7 thinc-8.1.10 typer-0.9.0 wasabi-1.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "88785b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple Inc. ORG\n",
      "California GPE\n",
      "United States GPE\n",
      "Steve Jobs PERSON\n",
      "Steve Wozniak PERSON\n",
      "April 1, 1976 DATE\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Example text\n",
    "text = \"Apple Inc. is a technology company based in California, United States. It was founded by Steve Jobs and Steve Wozniak on April 1, 1976.\"\n",
    "\n",
    "# Process the text using SpaCy's NLP pipeline\n",
    "doc = nlp(text)\n",
    "\n",
    "# Extract and display named entities\n",
    "for entity in doc.ents:\n",
    "    print(entity.text, entity.label_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d083999e",
   "metadata": {},
   "source": [
    "According to the output, Apple is an organization, 10000 is money, and\n",
    "New York is place. The results are accurate and can be used for any NLP\n",
    "applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a2de69",
   "metadata": {},
   "source": [
    "## 5. Extracting Topics from Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488a20aa",
   "metadata": {},
   "source": [
    "We are going to discuss how to identify topics from the\n",
    "document. Say, for example, there is an online library with multiple\n",
    "departments based on the kind of book. As the new book comes in,\n",
    "you want to look at the unique keywords/topics and decide on which\n",
    "department this book might belong to and place it accordingly. In these\n",
    "kinds of situations, topic modeling would be handy.\n",
    "Basically, this is document tagging and clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a645e6c9",
   "metadata": {},
   "source": [
    "### 5.1 Create the text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5ff18f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = \"I am learning NLP, it is very interesting and exciting.it includes machine learning and deep learning\"\n",
    "doc2 = \"My father is a data scientist and he is nlp expert\"\n",
    "doc3 = \"My sister has good exposure into android development\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0995282f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I am learning NLP, it is very interesting and exciting.it includes machine learning and deep learning',\n",
       " 'My father is a data scientist and he is nlp expert',\n",
       " 'My sister has good exposure into android development']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_complete = [doc1, doc2, doc3]\n",
    "doc_complete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f377a20c",
   "metadata": {},
   "source": [
    "### 5.2 Cleaning and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "281a5188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /Users/jothiramsanjeevi/opt/anaconda3/lib/python3.8/site-packages (3.8.3)\n",
      "Requirement already satisfied: six>=1.5.0 in /Users/jothiramsanjeevi/opt/anaconda3/lib/python3.8/site-packages (from gensim) (1.15.0)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /Users/jothiramsanjeevi/opt/anaconda3/lib/python3.8/site-packages (from gensim) (1.20.1)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /Users/jothiramsanjeevi/opt/anaconda3/lib/python3.8/site-packages (from gensim) (1.6.2)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/jothiramsanjeevi/opt/anaconda3/lib/python3.8/site-packages (from gensim) (6.3.0)\n"
     ]
    }
   ],
   "source": [
    "# Install and import libraries\n",
    "!pip install gensim\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "10d6d687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "584b2051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'being', 'just', 're', 'will', 'before', 'then', \"it's\", 'there', \"wasn't\", \"shouldn't\", 'now', 'in', 'after', 'same', 'shan', 'each', 'ma', \"mustn't\", 'too', \"should've\", 'again', 'hadn', \"that'll\", \"weren't\", 'such', 'into', \"hadn't\", 'he', 've', 'not', 'll', \"didn't\", 'own', 'have', 'only', 'is', 'should', 's', \"she's\", 'had', \"you'll\", 'an', 'haven', 'at', 'been', \"doesn't\", 'hasn', 'ours', 'shouldn', 'or', 'but', 'don', 'on', 'wouldn', 'has', 'yourselves', 'she', 'above', 'as', \"haven't\", 'weren', 'with', 'from', 'yourself', 'over', 'under', 'theirs', 'aren', 'won', 'be', 'which', \"couldn't\", \"mightn't\", 'while', 'myself', 'you', 'it', 'down', 'most', 'ourselves', 'these', 'herself', 'between', \"you've\", 'of', 'for', 'hers', 'few', 'were', 'more', 'them', 'why', 'where', \"hasn't\", \"isn't\", 'am', 'mustn', 'this', 'was', 'when', 'do', \"aren't\", 'are', 'how', 'didn', 'because', \"you're\", \"wouldn't\", 'their', 'other', 'him', 'that', 'against', 'during', 'o', 'does', 'my', 'himself', 'having', 'through', 'so', 'needn', 'until', \"won't\", 'themselves', 't', 'the', 'here', \"you'd\", 'i', 'her', 'and', \"shan't\", 'can', 'mightn', 'm', 'doesn', 'they', 'me', 'further', 'very', 'itself', 'what', 'about', 'some', 'yours', 'a', \"needn't\", 'if', 'y', 'nor', 'isn', 'who', 'ain', 'out', \"don't\", 'those', 'your', 'all', 'below', 'couldn', 'we', 'his', 'whom', 'to', 'both', 'off', 'no', 'wasn', 'by', 'our', 'its', 'any', 'd', 'doing', 'once', 'did', 'than', 'up'}\n",
      "{'?', '*', '>', '$', '~', '_', '/', '(', ']', ')', '\"', ',', ':', '\\\\', \"'\", '#', '%', ';', '.', '@', '{', '+', '!', '&', '[', '}', '-', '=', '^', '|', '`', '<'}\n",
      "<WordNetLemmatizer>\n"
     ]
    }
   ],
   "source": [
    "print(stop)\n",
    "print(exclude)\n",
    "print(lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "78796f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()\n",
    "def clean(doc):   \n",
    "    # Lowercase the document and split it into words\n",
    "    words = doc.lower().split()\n",
    "    \n",
    "    # Remove stopwords and punctuation marks, and lemmatize the words\n",
    "    cleaned_words = [lemma.lemmatize(word) for word in words if word not in stop and word not in exclude]\n",
    "    \n",
    "    # Join the cleaned words back into a single string\n",
    "    return \" \".join(cleaned_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "f7a2299b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am learning NLP, it is very interesting and exciting.it includes machine learning and deep learning'"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "b6500a0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'learning nlp, interesting exciting.it includes machine learning deep learning'"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "c37851bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'go'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma.lemmatize('goes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "f41ed050",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "953e66c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['learning', 'nlp,', 'interesting', 'exciting.it', 'includes', 'machine', 'learning', 'deep', 'learning'], ['father', 'data', 'scientist', 'nlp', 'expert'], ['sister', 'good', 'exposure', 'android', 'development']]\n"
     ]
    }
   ],
   "source": [
    "# Clean and normalize each sentence using the 'clean' function\n",
    "cleaned_sentences = [clean(sentence).split() for sentence in doc_complete]\n",
    "\n",
    "print(cleaned_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8ce485",
   "metadata": {},
   "source": [
    "### 5-3 Preparing document term matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef436688",
   "metadata": {},
   "source": [
    "A Document-Term Matrix (DTM) is a widely used representation of text data in Natural Language Processing (NLP) and Information Retrieval (IR). It is a mathematical matrix that describes the frequency of terms (words or phrases) that occur in a collection of documents. Each row of the matrix represents a document, and each column represents a term. The elements of the matrix are typically the term frequencies or some other measure of importance for the terms within the corresponding documents.\n",
    "\n",
    "The DTM is a fundamental data structure used in various text analysis tasks, including text classification, topic modeling, sentiment analysis, and more. It serves as the basis for many text-based machine learning models.\n",
    "\n",
    "Here's how a Document-Term Matrix is constructed:\n",
    "\n",
    "1. Tokenization: The text in each document is first tokenized into individual words or terms. Tokenization breaks the text into smaller units, typically words, but it can also be n-grams (combinations of n words).\n",
    "\n",
    "2. Vocabulary Creation: The vocabulary is the set of all unique terms that appear in the collection of documents. Each term becomes a column in the DTM.\n",
    "\n",
    "3. Counting Term Frequencies: For each document, the occurrence of each term is counted. The count represents how many times a specific term appears in that document.\n",
    "\n",
    "4. Assembling the Matrix: The DTM is constructed by arranging the term frequencies for each document in a matrix, where rows represent documents, and columns represent terms.\n",
    "\n",
    "Here's a simple example of a Document-Term Matrix:\n",
    "\n",
    "Consider the following three sentences:\n",
    "\n",
    "1. \"I love natural language processing.\"\n",
    "2. \"Text analysis is fascinating.\"\n",
    "3. \"Natural language processing is a subfield of NLP.\"\n",
    "\n",
    "Vocabulary: love, natural, language, processing, text, analysis, fascinating, is, a, subfield, of, NLP.\n",
    "\n",
    "DTM:\n",
    "\n",
    "| Document | love | natural | language | processing | text | analysis | fascinating | is | a | subfield | of | NLP |\n",
    "|----------|------|---------|----------|------------|------|----------|-------------|----|---|---------|----|-----|\n",
    "| Sentence 1 | 1    | 1        | 1         | 1           | 0    | 0        | 0            | 1   | 0 | 0       | 0  | 0   |\n",
    "| Sentence 2 | 0    | 0        | 0         | 0           | 1    | 1        | 1            | 1   | 0 | 0       | 0  | 0   |\n",
    "| Sentence 3 | 0    | 1        | 1         | 1           | 0    | 0        | 0            | 1   | 1 | 1       | 1  | 2   |\n",
    "\n",
    "In this example, the DTM represents the three sentences and the term frequencies for each term within each sentence. The DTM is a useful data representation for further text analysis and modeling tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "a4acb5cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 3), (5, 1), (6, 1)], [(7, 1), (8, 1), (9, 1), (10, 1), (11, 1)], [(12, 1), (13, 1), (14, 1), (15, 1), (16, 1)]]\n"
     ]
    }
   ],
   "source": [
    "# Importing gensim\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "# Creating the term dictionary of our corpus, where every unique term is assigned an index.\n",
    "dictionary = corpora.Dictionary(cleaned_sentences)\n",
    "# Converting a list of documents (corpus) into Document-TermMatrix using dictionary prepared above.\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in cleaned_sentences]\n",
    "print(doc_term_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c236226d",
   "metadata": {},
   "source": [
    "### 5.4 LDA model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae6dede",
   "metadata": {},
   "source": [
    "Latent Dirichlet Allocation (LDA) is a widely used probabilistic generative model in Natural Language Processing (NLP) and Machine Learning for topic modeling. LDA is used to discover hidden topics within a collection of documents by analyzing the distribution of words across these topics. It is an unsupervised learning algorithm that can automatically identify the main themes or topics present in the text data.\n",
    "\n",
    "The LDA model assumes that each document is a mixture of various topics, and each topic is a probability distribution over words. The goal of LDA is to infer the topic distribution for each document and the word distribution for each topic.\n",
    "\n",
    "Here's a high-level overview of how the LDA model works:\n",
    "\n",
    "1. Initialization: Choose the number of topics `k` you want to discover in the corpus of documents.\n",
    "\n",
    "2. Preprocessing: Prepare the text data by tokenizing the documents, removing stop words, and performing other text cleaning steps.\n",
    "\n",
    "3. Create the Document-Term Matrix (DTM): Represent the text data as a DTM, where rows represent documents, and columns represent terms (words).\n",
    "\n",
    "4. LDA Model Fitting: Apply the LDA algorithm to the DTM to find the topic distributions for each document and the word distributions for each topic. The algorithm uses probabilistic inference to estimate these distributions.\n",
    "\n",
    "5. Interpretation: Examine the output of the LDA model to understand the discovered topics. Each topic will have a distribution of words, and each document will have a distribution of topics. The most probable words in each topic can be considered the representative words of that topic.\n",
    "\n",
    "6. Topic Assignments: The LDA model assigns a probability distribution of topics to each document. It allows documents to be associated with multiple topics to varying degrees.\n",
    "\n",
    "Applications of LDA:\n",
    "- Topic Modeling: Discovering the main themes or topics within a collection of documents.\n",
    "- Document Clustering: Grouping similar documents based on their topic distributions.\n",
    "- Information Retrieval: Enhancing search results by associating documents with relevant topics.\n",
    "- Recommender Systems: Providing recommendations based on user preferences and document-topic associations.\n",
    "\n",
    "LDA is a powerful tool for unsupervised analysis of text data and is widely used in various domains to gain insights from large collections of documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9dc825a",
   "metadata": {},
   "source": [
    "Let's walk through an example of using the LDA model for topic modeling with sample texts. For simplicity, we'll use a smaller set of documents.\n",
    "\n",
    "Sample Texts:\n",
    "1. \"I love natural language processing.\"\n",
    "2. \"Text analysis is fascinating.\"\n",
    "3. \"Natural language processing is a subfield of NLP.\"\n",
    "4. \"Machine learning is an essential part of NLP.\"\n",
    "5. \"NLP applications are widespread in the industry.\"\n",
    "\n",
    "Now, let's use the LDA model to discover topics in these sample texts:\n",
    "\n",
    "Step 1: Preprocessing\n",
    "We'll preprocess the text data by tokenizing, converting to lowercase, and removing stopwords. We'll also remove single-character words and punctuations.\n",
    "\n",
    "Preprocessed Texts:\n",
    "1. [\"love\", \"natural\", \"language\", \"processing\"]\n",
    "2. [\"text\", \"analysis\", \"fascinating\"]\n",
    "3. [\"natural\", \"language\", \"processing\", \"subfield\", \"NLP\"]\n",
    "4. [\"machine\", \"learning\", \"essential\", \"part\", \"NLP\"]\n",
    "5. [\"NLP\", \"applications\", \"widespread\", \"industry\"]\n",
    "\n",
    "Step 2: Create Document-Term Matrix (DTM)\n",
    "The DTM will represent the text data, where each row corresponds to a document, and each column corresponds to a term (word).\n",
    "\n",
    "|            | love | natural | language | processing | text | analysis | fascinating | subfield | NLP | machine | learning | essential | part | applications | widespread | industry |\n",
    "|------------|------|---------|----------|------------|------|----------|-------------|----------|-----|---------|----------|-----------|------|--------------|-----------|----------|\n",
    "| Document 1 | 1    | 1       | 1        | 0          | 0    | 0        | 0           | 0        | 0   | 0       | 0        | 0         | 0    | 0            | 0         | 0        |\n",
    "| Document 2 | 0    | 0       | 0        | 1          | 1    | 1        | 0           | 0        | 0   | 0       | 0        | 0         | 0    | 0            | 0         | 0        |\n",
    "| Document 3 | 0    | 1       | 1        | 0          | 0    | 1        | 1           | 0        | 0   | 0       | 0        | 0         | 0    | 0            | 0         | 0        |\n",
    "| Document 4 | 0    | 0       | 0        | 0          | 0    | 0        | 0           | 1        | 1   | 1       | 0        | 0         | 0    | 0            | 0         | 0        |\n",
    "| Document 5 | 0    | 0       | 1        | 0          | 0    | 0        | 0           | 0        | 1   | 0       | 0        | 1         | 1    | 1            | 1         | 1        |\n",
    "\n",
    "\n",
    "Step 3: Train the LDA Model\n",
    "We'll use the DTM to train the LDA model and discover topics in the sample texts.\n",
    "\n",
    "Step 4: Interpretation\n",
    "The LDA model will provide us with topic distributions for each document and word distributions for each topic. We can then interpret the topics based on the most probable words in each topic.\n",
    "\n",
    "In this example, the LDA model may discover topics like:\n",
    "- Topic 1: \"NLP, natural language processing, subfield\"\n",
    "- Topic 2: \"Text analysis, fascinating\"\n",
    "- Topic 3: \"Machine learning, essential part\"\n",
    "- Topic 4: \"Applications, widespread, industry\"\n",
    "\n",
    "Keep in mind that the number of topics and their interpretations depend on the input data and the hyperparameters chosen for the LDA model. The goal of the LDA model is to identify topics that capture the main themes present in the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "afd96e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.125*\"sister\" + 0.125*\"good\" + 0.125*\"exposure\" + 0.125*\"development\" + 0.125*\"android\" + 0.031*\"father\" + 0.031*\"scientist\" + 0.031*\"nlp\" + 0.031*\"expert\" + 0.031*\"data\"'), (1, '0.125*\"scientist\" + 0.125*\"data\" + 0.125*\"father\" + 0.125*\"expert\" + 0.125*\"nlp\" + 0.031*\"good\" + 0.031*\"exposure\" + 0.031*\"development\" + 0.031*\"android\" + 0.031*\"sister\"'), (2, '0.227*\"learning\" + 0.091*\"exciting.it\" + 0.091*\"machine\" + 0.091*\"deep\" + 0.091*\"nlp,\" + 0.091*\"interesting\" + 0.091*\"includes\" + 0.023*\"exposure\" + 0.023*\"android\" + 0.023*\"good\"')]\n"
     ]
    }
   ],
   "source": [
    "# Creating the object for LDA model using gensim library\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "# Running and Training LDA model on the document term matrixfor 3 topics.\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=3, id2word =dictionary, passes=50)\n",
    "# Results\n",
    "print(ldamodel.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "06c22def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: 0.155*\"learning\" + 0.067*\"nlp\" + 0.067*\"data\" + 0.067*\"scientist\" + 0.067*\"expert\" + 0.067*\"father\" + 0.067*\"deep\" + 0.067*\"exciting.it\" + 0.066*\"nlp,\" + 0.066*\"machine\"\n",
      "Topic 2: 0.110*\"development\" + 0.110*\"sister\" + 0.110*\"android\" + 0.110*\"good\" + 0.110*\"exposure\" + 0.039*\"learning\" + 0.038*\"interesting\" + 0.038*\"includes\" + 0.038*\"machine\" + 0.038*\"nlp,\"\n"
     ]
    }
   ],
   "source": [
    "# Train the LDA model\n",
    "num_topics = 2  # Number of topics to discover\n",
    "lda_model = gensim.models.LdaModel(doc_term_matrix, num_topics=num_topics, id2word=dictionary, passes=10)\n",
    "\n",
    "# Print the discovered topics and their most probable words\n",
    "for topic_id in range(num_topics):\n",
    "    print(f\"Topic {topic_id + 1}: {lda_model.print_topic(topic_id)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928f5d3b",
   "metadata": {},
   "source": [
    "All the weights associated with the topics from the sentence seem\n",
    "almost similar. You can perform this on huge data to extract significant\n",
    "topics. The whole idea to implement this on sample data is to make you\n",
    "familiar with it, and you can use the same code snippet to perform on the\n",
    "huge data for significant results and insights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadb25ce",
   "metadata": {},
   "source": [
    "## 6. Classifying Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fe2267",
   "metadata": {},
   "source": [
    "Text classification – The aim of text classification is to automatically classifythe text documents based on pretrained categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11d9a8a",
   "metadata": {},
   "source": [
    "Applications:\n",
    "- Sentiment Analysis\n",
    "- Document classification\n",
    "- Spam – ham mail classification\n",
    "- Resume shortlisting\n",
    "- Document summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e2dd6f",
   "metadata": {},
   "source": [
    "Spam - ham classification using machine learning\n",
    "\"Spam\" and \"ham\" are terms commonly used in the context of email classification and spam filtering. They refer to two categories of emails based on their content.\n",
    "\n",
    "1. Spam:\n",
    "Spam refers to unsolicited or unwanted emails, typically sent in bulk to a large number of recipients. These emails often contain promotional content, advertisements, scams, or other forms of irrelevant and potentially harmful messages. The primary purpose of sending spam emails is usually to promote products, services, or fraudulent activities.\n",
    "\n",
    "2. Ham:\n",
    "Ham, on the other hand, refers to legitimate and non-spam emails. These are the desired emails that users expect to receive from known and trusted sources, such as personal or work-related emails, newsletters from subscribed services, and communication from friends, colleagues, or organizations.\n",
    "\n",
    "Spam filtering is a technique used to automatically identify and separate spam emails from legitimate ones (ham) in an email inbox. Machine learning algorithms, such as Naive Bayes, Support Vector Machines, and Deep Learning models, are commonly used for spam filtering tasks. These algorithms analyze the content and characteristics of emails to classify them as either spam or ham.\n",
    "\n",
    "By using spam filters, email providers and users can help reduce the annoyance and potential risks associated with receiving unwanted or harmful spam emails, ensuring that legitimate emails reach the inbox while unwanted spam is filtered out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01eeb18",
   "metadata": {},
   "source": [
    "If you observe, your Gmail has a folder called “Spam.” It will basically\n",
    "classify your emails into spam and ham so that you don’t have to read\n",
    "unnecessary emails."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b767f4",
   "metadata": {},
   "source": [
    "### 6.1 Data collection and understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a422e224",
   "metadata": {},
   "source": [
    "Please download data from the below link and save it in your working\n",
    "directory:<br>\n",
    "\n",
    "https://www.kaggle.com/uciml/sms-spam-collection-dataset#spam.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "c01c2208",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['v1', 'v2', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], dtype='object')"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#Read the data\n",
    "Email_Data = pd.read_csv(\"spam.csv\",encoding ='latin1')\n",
    "#Data undestanding\n",
    "Email_Data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "c9d05ee2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will Ì_ b going to esplanade fr home?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        v1                                                 v2 Unnamed: 2  \\\n",
       "0      ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
       "1      ham                      Ok lar... Joking wif u oni...        NaN   \n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
       "3      ham  U dun say so early hor... U c already then say...        NaN   \n",
       "4      ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
       "...    ...                                                ...        ...   \n",
       "5567  spam  This is the 2nd time we have tried 2 contact u...        NaN   \n",
       "5568   ham              Will Ì_ b going to esplanade fr home?        NaN   \n",
       "5569   ham  Pity, * was in mood for that. So...any other s...        NaN   \n",
       "5570   ham  The guy did some bitching but I acted like i'd...        NaN   \n",
       "5571   ham                         Rofl. Its true to its name        NaN   \n",
       "\n",
       "     Unnamed: 3 Unnamed: 4  \n",
       "0           NaN        NaN  \n",
       "1           NaN        NaN  \n",
       "2           NaN        NaN  \n",
       "3           NaN        NaN  \n",
       "4           NaN        NaN  \n",
       "...         ...        ...  \n",
       "5567        NaN        NaN  \n",
       "5568        NaN        NaN  \n",
       "5569        NaN        NaN  \n",
       "5570        NaN        NaN  \n",
       "5571        NaN        NaN  \n",
       "\n",
       "[5572 rows x 5 columns]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Email_Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "9711e9be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>Email</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Target                                              Email\n",
       "0    ham  Go until jurong point, crazy.. Available only ...\n",
       "1    ham                      Ok lar... Joking wif u oni...\n",
       "2   spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3    ham  U dun say so early hor... U c already then say...\n",
       "4    ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Email_Data = Email_Data[['v1', 'v2']]\n",
    "Email_Data = Email_Data.rename(columns={\"v1\":\"Target\",\"v2\":\"Email\"})\n",
    "Email_Data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172da9d4",
   "metadata": {},
   "source": [
    "### 6.2 Text processing and feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "0a621761",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from textblob import TextBlob\n",
    "from nltk.stem import PorterStemmer\n",
    "from textblob import Word\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "import sklearn.feature_extraction.text as text\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "525b6800",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre processing steps like lower case, stemming and lemmatization\n",
    "Email_Data['Email'] = Email_Data['Email'].apply(lambda x:\" \".join(x.lower() for x in x.split()))\n",
    "stop = stopwords.words('english')\n",
    "Email_Data['Email'] = Email_Data['Email'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "st = PorterStemmer()\n",
    "Email_Data['Email'] = Email_Data['Email'].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))\n",
    "Email_Data['Email'] = Email_Data['Email'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "78bf6f17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'go jurong point, crazy.. avail bugi n great world la e buffet... cine got amor wat...'"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Email_Data['Email'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "3b1a5308",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting data into train and validation\n",
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(Email_Data['Email'], Email_Data['Target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7654bcad",
   "metadata": {},
   "source": [
    " ### 6.3 Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa8f48b",
   "metadata": {},
   "source": [
    "In the context of TF-IDF vectorization, the terms \"analyzer\" and \"token pattern\" are important concepts that relate to how text is processed and tokenized before creating the TF-IDF representation.\n",
    "\n",
    "1. Analyzer:\n",
    "The analyzer is a component in TF-IDF vectorization that defines how the text data should be preprocessed and tokenized before generating the TF-IDF representation. It essentially determines the unit of analysis, which can be at the word level, character level, or any other custom level.\n",
    "\n",
    "The default analyzer in scikit-learn's TfidfVectorizer is `word`, which means that the input text will be tokenized into individual words. For example, the sentence \"I love natural language processing\" will be tokenized into the following individual words: [\"I\", \"love\", \"natural\", \"language\", \"processing\"].\n",
    "\n",
    "You can also specify a custom analyzer by providing your own function. For instance, you could create a custom analyzer that tokenizes the text at the character level, or applies additional preprocessing steps like stemming or lemmatization before tokenization.\n",
    "\n",
    "2. Token Pattern:\n",
    "The token pattern is a regular expression (regex) that specifies what constitutes a \"token\" during tokenization. It defines the rules for breaking the text into smaller units, which are then considered as individual tokens.\n",
    "\n",
    "By default, the token pattern in scikit-learn's TfidfVectorizer is `r\"(?u)\\b\\w\\w+\\b\"`. This pattern means that tokens are formed by sequences of two or more word characters (letters, digits, or underscores) bounded by word boundaries. The `(?u)` is used to indicate that Unicode word boundaries should be used, allowing tokenization of words with non-ASCII characters.\n",
    "\n",
    "For example, with the default token pattern, the sentence \"Text analysis is fascinating\" will be tokenized into the following words: [\"Text\", \"analysis\", \"is\", \"fascinating\"].\n",
    "\n",
    "You can customize the token pattern to suit your specific requirements. For instance, if you want to include single-letter words, you can modify the pattern to `r\"(?u)\\b\\w+\\b\"`. If you want to consider phrases as tokens, you can use a different regex pattern to match phrases based on your desired criteria.\n",
    "\n",
    "In summary, the analyzer and token pattern in TF-IDF vectorization allow you to control how text data is preprocessed, tokenized, and converted into a numerical representation. These options are essential for fine-tuning the TF-IDF vectorization process to best suit your specific text analysis needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "a77abce8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.508842  , 0.73734716, 0.44429605, ..., 0.19891341, 0.09118786,\n",
       "       0.20414672])"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TFIDF feature generation for a maximum of 5000 features\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word',token_pattern=r'\\w{1,}', max_features=5000)\n",
    "tfidf_vect.fit(Email_Data['Email'])\n",
    "xtrain_tfidf = tfidf_vect.transform(train_x)\n",
    "xvalid_tfidf = tfidf_vect.transform(valid_x)\n",
    "xtrain_tfidf.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bafb5cb",
   "metadata": {},
   "source": [
    "### 6.3 Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae6c216",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "c40d44b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(classifier, feature_vector_train, label,feature_vector_valid, is_neural_net=False):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    return metrics.accuracy_score(predictions, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "e1a7cfca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9856424982053122\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes trainig\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(alpha=0.2),xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print (\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "c1dab071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9641062455132807\n"
     ]
    }
   ],
   "source": [
    "# Linear Classifier on Word Level TF IDF Vectors\n",
    "accuracy = train_model(linear_model.LogisticRegression(),xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print (\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f57b01",
   "metadata": {},
   "source": [
    "Naive Bayes is giving better results than the linear classifier. We can try\n",
    "many more classifiers and then choose the best one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3557147b",
   "metadata": {},
   "source": [
    "## 7. Carrying Out Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51fc97a",
   "metadata": {},
   "source": [
    "We are going to discuss how to understand the sentiment of\n",
    "a particular sentence or statement. Sentiment analysis is one of the widely\n",
    "used techniques across the industries to understand the sentiments of the\n",
    "customers/users around the products/services. Sentiment analysis gives\n",
    "the sentiment score of a sentence/statement tending toward positive or\n",
    "negative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982e6e26",
   "metadata": {},
   "source": [
    "The simplest way to do this by using a **TextBlob** or **vedar** library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd66bef",
   "metadata": {},
   "source": [
    "Let’s follow the steps in this section to do sentiment analysis using\n",
    "TextBlob. It will basically give 2 metrics.\n",
    "- Polarity = Polarity lies in the range of [-1,1] where 1 means a positive statement and -1 means a negative statement.\n",
    "- Subjectivity = Subjectivity refers that mostly it is a public opinion and not factual information [0,1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "b1b3b40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "review = \"I like this phone. screen quality and camera clarity is really good.\"\n",
    "review2 = \"This tv is not good. Bad quality, no clarity, worste xperience\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "7effb1f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=-0.5249999999999999, subjectivity=0.6333333333333333)"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import libraries\n",
    "from textblob import TextBlob\n",
    "#TextBlob has a pre trained sentiment prediction model\n",
    "blob = TextBlob(review2)\n",
    "blob.sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69223e1",
   "metadata": {},
   "source": [
    "This is a negative review, as the polarity is “-0.52.”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d673c464",
   "metadata": {},
   "source": [
    "## 8. Disambiguating Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79526832",
   "metadata": {},
   "source": [
    "There is ambiguity that arises due to a different meaning of words in a\n",
    "different context.\n",
    "For example,\n",
    "\n",
    "- Text1 = 'I went to the bank to deposit my money'\n",
    "- Text2 = 'The river bank was full of dead fishes'\n",
    "\n",
    "In the above texts, the word “bank” has different meanings based on\n",
    "the context of the sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecc3a22",
   "metadata": {},
   "source": [
    "The Lesk algorithm is one of the best algorithms for word sense\n",
    "disambiguation. Let’s see how to solve using the package pywsd and nltk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a9ea32",
   "metadata": {},
   "source": [
    "The Lesk Algorithm is a word sense disambiguation (WSD) algorithm used to determine the correct sense of an ambiguous word within a given context. It was proposed by Michael E. Lesk in 1986. Word sense disambiguation is an important task in natural language processing (NLP) to resolve the ambiguity that arises when a word has multiple meanings or senses depending on its context.\n",
    "\n",
    "The Lesk Algorithm is based on the idea that words in a given context tend to share similar words in their definitions. It utilizes the glosses (short definitions) of different word senses from a lexical database, such as WordNet, and compares these glosses with the words in the context of the ambiguous word.\n",
    "\n",
    "Here's a high-level overview of the Lesk Algorithm:\n",
    "\n",
    "1. Given an ambiguous word and its context sentence.\n",
    "2. Retrieve the glosses (short definitions) of all possible word senses of the ambiguous word from a lexical database like WordNet.\n",
    "3. Tokenize the context sentence to obtain individual words.\n",
    "4. For each word in the context sentence, calculate the overlap between the glosses of each word sense and the set of words in the context. The overlap is determined using set intersection or other similarity metrics.\n",
    "5. Choose the word sense with the highest overlap as the disambiguated sense for the ambiguous word.\n",
    "\n",
    "By comparing the glosses of different word senses with the words in the context, the Lesk Algorithm aims to find the most relevant sense that best fits the context.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The Lesk Algorithm has been widely used in NLP tasks, especially in the field of lexical semantics and word sense disambiguation. While the algorithm is relatively simple, it provides a practical and effective approach for resolving word sense ambiguity in various applications, such as machine translation, information retrieval, and question answering systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c856d665",
   "metadata": {},
   "source": [
    "Step-by-step pictorial representation of the Lesk Algorithm with a simple example to help you visualize the process.\n",
    "\n",
    "Let's consider the word \"bank,\" which can have multiple senses (meanings) depending on the context:\n",
    "\n",
    "1. Bank (financial institution)\n",
    "2. Bank (side of a river)\n",
    "\n",
    "Context Sentence: \"I went to the bank to deposit some money.\"\n",
    "\n",
    "- Step 1: Get Synsets from WordNet\n",
    "Retrieve all possible word senses (synsets) of the word \"bank\" from WordNet.\n",
    "\n",
    "- Step 2: Get Glosses (Short Definitions) Retrieve the glosses (short definitions) for each synset.\n",
    "\n",
    "```\n",
    "Synset 1 (Bank - financial institution):\n",
    "    Gloss: \"a financial establishment that uses money deposited by customers for investment, pays it out when required, makes loans at interest, and exchanges currency.\"\n",
    "\n",
    "Synset 2 (Bank - side of a river):\n",
    "    Gloss: \"sloping land (especially the slope beside a body of water).\"\n",
    "```\n",
    "\n",
    "- Step 3: Tokenize the Context Sentence Tokenize the words in the context sentence to obtain individual words.\n",
    "\n",
    "Context Tokens: [\"I\", \"went\", \"to\", \"the\", \"bank\", \"to\", \"deposit\", \"some\", \"money\"]\n",
    "\n",
    "- Step 4: Calculate Overlap For each synset's gloss, calculate the overlap with the words in the context sentence. The overlap can be determined using set intersection or other similarity metrics.\n",
    "\n",
    "```\n",
    "Overlap with Synset 1:\n",
    "    \"bank\" and \"financial\" - 1 word match\n",
    "    \"bank\" and \"institution\" - 1 word match\n",
    "    Total overlap: 2\n",
    "\n",
    "Overlap with Synset 2:\n",
    "    \"bank\" and \"sloping\" - 0 word match\n",
    "    \"bank\" and \"land\" - 1 word match\n",
    "    \"bank\" and \"side\" - 1 word match\n",
    "    \"bank\" and \"river\" - 0 word match\n",
    "    Total overlap: 2\n",
    "```\n",
    "\n",
    "- Step 5: Choose the Best Sense\n",
    "Select the synset with the highest overlap as the disambiguated sense for the ambiguous word \"bank.\" In this example, both Synset 1 and Synset 2 have the same overlap (2 words), so we may need further context or disambiguation techniques to make the final decision.\n",
    "\n",
    "In more complex examples, the Lesk Algorithm considers more words and senses, and the overlap calculations become more extensive. However, this simplified example illustrates the core idea of how the Lesk Algorithm works to disambiguate an ambiguous word based on its context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2516dea0",
   "metadata": {},
   "source": [
    "### 8.1 Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "4ec9289c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pywsd\n",
      "  Downloading pywsd-1.2.5-py3-none-any.whl (26.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 26.9 MB 816 kB/s eta 0:00:01     |██████████████████████████▍     | 22.2 MB 1.7 MB/s eta 0:00:03     |███████████████████████████▏    | 22.9 MB 2.5 MB/s eta 0:00:02\n",
      "\u001b[?25hRequirement already satisfied: nltk in /Users/jothiramsanjeevi/opt/anaconda3/lib/python3.8/site-packages (from pywsd) (3.6.1)\n",
      "Collecting wn==0.0.23\n",
      "  Downloading wn-0.0.23.tar.gz (31.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 31.6 MB 403 kB/s eta 0:00:01    |▌                               | 542 kB 2.2 MB/s eta 0:00:15     |███████▋                        | 7.5 MB 1.6 MB/s eta 0:00:16     |████████████▏                   | 12.0 MB 2.1 MB/s eta 0:00:10     |██████████████▉                 | 14.7 MB 2.2 MB/s eta 0:00:08     |█████████████████▊              | 17.5 MB 2.2 MB/s eta 0:00:07     |███████████████████▊            | 19.5 MB 495 kB/s eta 0:00:25     |██████████████████████████▋     | 26.3 MB 985 kB/s eta 0:00:06\n",
      "\u001b[?25hRequirement already satisfied: numpy in /Users/jothiramsanjeevi/opt/anaconda3/lib/python3.8/site-packages (from pywsd) (1.20.1)\n",
      "Requirement already satisfied: six in /Users/jothiramsanjeevi/opt/anaconda3/lib/python3.8/site-packages (from pywsd) (1.15.0)\n",
      "Requirement already satisfied: pandas in /Users/jothiramsanjeevi/opt/anaconda3/lib/python3.8/site-packages (from pywsd) (1.2.4)\n",
      "Requirement already satisfied: tqdm in /Users/jothiramsanjeevi/opt/anaconda3/lib/python3.8/site-packages (from nltk->pywsd) (4.59.0)\n",
      "Requirement already satisfied: click in /Users/jothiramsanjeevi/opt/anaconda3/lib/python3.8/site-packages (from nltk->pywsd) (7.1.2)\n",
      "Requirement already satisfied: joblib in /Users/jothiramsanjeevi/opt/anaconda3/lib/python3.8/site-packages (from nltk->pywsd) (1.0.1)\n",
      "Requirement already satisfied: regex in /Users/jothiramsanjeevi/opt/anaconda3/lib/python3.8/site-packages (from nltk->pywsd) (2021.4.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/jothiramsanjeevi/opt/anaconda3/lib/python3.8/site-packages (from pandas->pywsd) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /Users/jothiramsanjeevi/opt/anaconda3/lib/python3.8/site-packages (from pandas->pywsd) (2021.1)\n",
      "Building wheels for collected packages: wn\n",
      "  Building wheel for wn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wn: filename=wn-0.0.23-py3-none-any.whl size=31792943 sha256=2cae08b34d6ac28058e57ea0f14bd49688dbd37b8364efd6735bcbcd70988686\n",
      "  Stored in directory: /Users/jothiramsanjeevi/Library/Caches/pip/wheels/6b/eb/fe/eb7c7be28c29ee90dd9d6f58c116673d0eb07b2d83dfb72a37\n",
      "Successfully built wn\n",
      "Installing collected packages: wn, pywsd\n",
      "Successfully installed pywsd-1.2.5 wn-0.0.23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warming up PyWSD (takes ~10 secs)... took 5.101330995559692 secs.\n"
     ]
    }
   ],
   "source": [
    "#First, import the libraries:\n",
    "#Install pywsd\n",
    "!pip install pywsd\n",
    "#Import functions\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import PorterStemmer\n",
    "from itertools import chain\n",
    "from pywsd.lesk import simple_lesk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58860c8e",
   "metadata": {},
   "source": [
    "### 8.2 Disambiguating word sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "6e2c819a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context-1: I went to the bank to deposit my money\n",
      "Sense: Synset('depository_financial_institution.n.01')\n",
      "Definition :  a financial institution that accepts deposits and channels the money into lending activities\n",
      "Context-2: The river bank was full of dead fishes\n",
      "Sense: Synset('bank.n.01')\n",
      "Definition :  sloping land (especially the slope beside a body of water)\n"
     ]
    }
   ],
   "source": [
    "# Sentences\n",
    "bank_sents = ['I went to the bank to deposit my money','The river bank was full of dead fishes']\n",
    "# calling the lesk function and printing results for both the\n",
    "sentences\n",
    "print (\"Context-1:\", bank_sents[0])\n",
    "answer = simple_lesk(bank_sents[0],'bank')\n",
    "print (\"Sense:\", answer)\n",
    "print (\"Definition : \", answer.definition())\n",
    "print (\"Context-2:\", bank_sents[1])\n",
    "answer = simple_lesk(bank_sents[1],'bank','n')\n",
    "print (\"Sense:\", answer)\n",
    "print (\"Definition : \", answer.definition())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a73ca0",
   "metadata": {},
   "source": [
    "Observe that in context-1, “bank” is a financial institution, but in\n",
    "context-2, “bank” is sloping land"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4507c72f",
   "metadata": {},
   "source": [
    "## 9. Converting Speech to Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafd1af4",
   "metadata": {},
   "source": [
    "The simplest way to do this by using Speech Recognition and PyAudio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e60123",
   "metadata": {},
   "source": [
    "Interaction with machines is trending toward the voice, which is the usual\n",
    "way of human communication. Popular examples are Siri, Alexa’s Google\n",
    "Voice, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "d382ce63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: SpeechRecognition in /Users/jothiramsanjeevi/opt/anaconda3/lib/python3.8/site-packages (3.10.0)\n",
      "Requirement already satisfied: requests>=2.26.0 in /Users/jothiramsanjeevi/opt/anaconda3/lib/python3.8/site-packages (from SpeechRecognition) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/jothiramsanjeevi/opt/anaconda3/lib/python3.8/site-packages (from requests>=2.26.0->SpeechRecognition) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/jothiramsanjeevi/opt/anaconda3/lib/python3.8/site-packages (from requests>=2.26.0->SpeechRecognition) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/jothiramsanjeevi/opt/anaconda3/lib/python3.8/site-packages (from requests>=2.26.0->SpeechRecognition) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jothiramsanjeevi/opt/anaconda3/lib/python3.8/site-packages (from requests>=2.26.0->SpeechRecognition) (2020.12.5)\n",
      "\u001b[33mDEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: PyAudio in /Users/jothiramsanjeevi/opt/anaconda3/lib/python3.8/site-packages (0.2.11)\n",
      "\u001b[33mDEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install SpeechRecognition\n",
    "!pip install PyAudio\n",
    "import speech_recognition as sr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1222f5dd",
   "metadata": {},
   "source": [
    "If you are using Mac machine you may face problem while installing the Pyaudio use the below code in the terminal to install the package"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602789f6",
   "metadata": {},
   "source": [
    "Now after you run the below code snippet, whatever you say on the\n",
    "microphone (using recognize_google function) gets converted into text.\n",
    "\n",
    "<span style=\"color:red;\">conda install -c anaconda pyaudio</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "2509c685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please say something\n",
      "Time over, thanks\n",
      "I think you said: hello how are you\n"
     ]
    }
   ],
   "source": [
    "r=sr.Recognizer()\n",
    "with sr.Microphone() as source:\n",
    "    print(\"Please say something\")\n",
    "    audio = r.listen(source)\n",
    "    print(\"Time over, thanks\")\n",
    "try:\n",
    "    print(\"I think you said: \"+r.recognize_google(audio));\n",
    "except:\n",
    "    pass;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2446c36",
   "metadata": {},
   "source": [
    "## 10. Converting Text to Speech"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723fe4e3",
   "metadata": {},
   "source": [
    "Converting text to speech is another useful NLP technique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0f5f6d",
   "metadata": {},
   "source": [
    "The simplest way to do this by using the gTTs library.\n",
    "\n",
    "\n",
    "The \"gTTS\" (Google Text-to-Speech) library is a Python library and CLI tool that allows you to convert text into speech using Google Text-to-Speech API. It provides a simple interface to generate speech from text, and it supports multiple languages and various voice options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "df3ee8ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gTTS\n",
      "  Downloading gTTS-2.3.2-py3-none-any.whl (28 kB)\n",
      "Requirement already satisfied: requests<3,>=2.27 in /Users/jothiramsanjeevi/opt/anaconda3/lib/python3.8/site-packages (from gTTS) (2.31.0)\n",
      "Requirement already satisfied: click<8.2,>=7.1 in /Users/jothiramsanjeevi/opt/anaconda3/lib/python3.8/site-packages (from gTTS) (7.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/jothiramsanjeevi/opt/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.27->gTTS) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/jothiramsanjeevi/opt/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.27->gTTS) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/jothiramsanjeevi/opt/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.27->gTTS) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jothiramsanjeevi/opt/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.27->gTTS) (2020.12.5)\n",
      "\u001b[33mDEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: gTTS\n",
      "Successfully installed gTTS-2.3.2\n"
     ]
    }
   ],
   "source": [
    "!pip install gTTS\n",
    "from gtts import gTTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "130750d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /Users/jothiramsanjeevi/opt/anaconda3/lib/python3.8/site-packages (23.2.1)\n",
      "\u001b[33mDEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273448e8",
   "metadata": {},
   "source": [
    "Now after you run the below code snippet, whatever you input in the text parameter gets converted into audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "c08f2b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gtts import gTTS\n",
    "import os\n",
    "\n",
    "# Text to convert to speech\n",
    "text = \"LWM tamil youtube channel is the best in the world!\"\n",
    "# Create a gTTS object\n",
    "tts = gTTS(text=text, lang='en')\n",
    "\n",
    "# Save the speech as an audio file\n",
    "tts.save(\"output.mp3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462aa5ad",
   "metadata": {},
   "source": [
    "## 11. Translating Speech"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae4511b",
   "metadata": {},
   "source": [
    "Whenever you try to analyze data from blogs that are hosted across the\n",
    "globe, especially websites from countries like China, where Chinese is\n",
    "used predominantly, analyzing such data or performing NLP tasks on such\n",
    "data would be difficult. That’s where language translation comes to the rescue. You want to translate one language to another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "32ffca2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting goslate\n",
      "  Downloading goslate-1.5.4.tar.gz (14 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting futures (from goslate)\n",
      "  Downloading futures-3.0.5.tar.gz (25 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: goslate, futures\n",
      "  Building wheel for goslate (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for goslate: filename=goslate-1.5.4-py3-none-any.whl size=11578 sha256=6882b7f70e695825922fd148e8d9335a84b4e19017c9fbf97a3bd49c4f621f9e\n",
      "  Stored in directory: /Users/jothiramsanjeevi/Library/Caches/pip/wheels/47/39/0e/e5f72817087159cd0bb36bbe09b12f5e9ae39af8b10aaef8fa\n",
      "  Building wheel for futures (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for futures: filename=futures-3.0.5-py3-none-any.whl size=14068 sha256=64e29cfd8fa7728461ca0245bc7ce5523f7f8f5b7f3df338d61bab5e8d5288b4\n",
      "  Stored in directory: /Users/jothiramsanjeevi/Library/Caches/pip/wheels/8b/63/2a/b26b5d293191299916236972878f61ed040ae952349024bb56\n",
      "Successfully built goslate futures\n",
      "\u001b[33mDEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: futures, goslate\n",
      "Successfully installed futures-3.0.5 goslate-1.5.4\n"
     ]
    }
   ],
   "source": [
    "!pip install goslate\n",
    "import goslate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "af0d52a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"LWM ist das beste YouTube Kanal der Welt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab2c15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = goslate.Goslate()\n",
    "translatedText = gs.translate(text,'en')\n",
    "print(translatedText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3183d28f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
